{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文学习于论文《Get To The Point: Summarization with Pointer-Generator Networks》，作者：Abigail See"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经序列到序列模型(seq2seq)为生成式文本摘要提供了一种可行的新方法（这意味着它们不仅限于简单地从原始文本中选择和重新排列段落）。然而，这些模型有两个缺点：它们易于不准确地再现事实细节，并且往往会重复。在本文中，我们提出了一种新的架构，它以两种正交方式扩充了标准的序列到序列注意力模型。首先，我们使用一个混合的指针生成网络，它可以通过指针从源文本中复制单词，这有助于信息的精确再现，同时保持通过生成器生成新单词的能力。其次，我们使用`coverage`来跟踪已总结的摘要内容，这会阻止重复。我们将模型应用于`CNN/Daily`摘要任务，性能超过最先进技术至少2个`Rouge`点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "摘要是将一段文本任务压缩为较短版本的任务，其中包含来自原始文本的主要信息。它有两种广泛的方法：抽取式摘要和生成式摘要。抽取式摘要是直接从原文的段落(通常是整句)中收集摘要，而生成式摘要可能会生成不在原文中的新词和短语--就像人类书写的摘要。抽取式摘要更容易，因为从源文档复制大块文本可以确保语法性和准确性的基线水平。另一方面，只有在抽象的框架中，才能获得对高质量摘要至关重要的复杂能力，比如解释、概括或结合现实世界的知识。\n",
    "\n",
    "由于生成式摘要的难度，绝大多数的研究都是抽取式摘要（Horacio Saggion《Automatic text summarization: Past, present and future》）。然而，近来序列到序列模型（Ilya Sutskever《Sequence to sequence learning with neural networks》）的成功，其中RNN既读取又自由生成文本，使得生成式摘要（Sumit Chopra《Abstractive sentence summarization with attentive recurrent neural networks》）变得可行。虽然这些系统很有前景，但它们表现出不良行为，例如不准确地再现事实细节，无法处理词包外的词(OOV)，以及重复自身，如下图所示：\n",
    "\n",
    "![](./image/pointer_network_seq2seq_attention.png)\n",
    "\n",
    "在本文中，我们提出了一种架构，它可以在多句摘要的情况下解决这三个问题。虽然最近的生成式摘要工作侧重于标题生成任务（将一个或两个句子简化为一个标题），但我们认为，较长的文本摘要不仅具有挑战性（需要更高的抽象级别，同时避免重复），而且最终更有用。因此，我们将模型应用于CNN/Daily Mail数据集（Ramesh Nallapati《Abstractive text summarization using sequence-to-sequence RNNs and beyond》），它包含新闻文章的多句摘要，并且在性能上超过最先进的生成式摘要系统至少2个ROUGE点。\n",
    "\n",
    "我们的混合指针生成器网络(hybrid pointer-generator network)有助于通过指针（Oriol Vinyals《Pointer networks》）从源文本中复制单词，这提高了准确性和处理OOV单词的能力，同时保留了生成新单词的能力。该网络可以被视为抽取式方法和生成式方法之间的平衡，类似于的Jiatao Gu的《Incorporating copying mechanism in sequence-to-sequence learning》的CopyNet和 Yishu Miao的《Language as a latent variable: Discrete generative models for sentence compression》的Forced-Attention Sentence Compression,它们被应用于短文本摘要。我们从神经机器翻译中提出了一种新的coverage vector变体（Zhaopeng Tu《Modeling coverage for neural machine translation》），用它来追踪和控制源文档的覆盖范围。结果表明，coverage对消除重复非常有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我们的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这部分，我们在`(1)`中描述了作为基准的sequence-to-sequence模型，在`(2)`中描述了我们的 指针-生成器网络，在`(3)` 中描述了 `coverage机制`，它可以被添加到前两个模型中的任何一个。模型代码位于www.github.com/abisee/pointer-generator 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列到序列注意力模型：Sequence-to-sequence attentional model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的基准`seq2seq 模型`与Ramesh Nallapati的论文《Abstractive text summarization using sequence-to-sequence RNNs and beyond》相似，具体描述如图2所示。文章 $w_{i}$ 的符号（字或词）一个接一个的灌进编码器(一个单层的双向LSTM)，然后产生一个编码器隐藏状态 $h_{i}$的序列。在每一步 $t$,解码器(一个单层的单向LSTM)接受前面单词的词嵌入(训练时，这是参考摘要的前一个词；预测时，它是解码器发出的前一个字)，还有一个解码状态 $s_{t}$。注意力分布$a_{t}$的计算与论文Dzmitry Bahdanau《Neural machine translation by jointly learning to align and translate》一样：\n",
    "\n",
    "$$e_{i}^{t} = v^{T}tanh\\left(W_{h}h_{i} + W_{s}s_{t} + b_{atten}\\right)　　　(1)$$\n",
    "$$a^{t} = softmax\\left(e^{t}\\right)　　　(2)$$\n",
    "\n",
    "这里$v,W_{h},W_{s},b_{atten}$ 是可学习的参数。**注意力分布可以视为在源单词上的概率分布，它告诉解码器应该查看编码器的哪一时间步来生成下一个单词**。接下来，注意力分布被用来产生编码器隐藏状态的加权和，即**上下文向量** $h_{t}^{*}$:\n",
    "\n",
    "$$h_{t}^{*} = \\sum_{i} a_{i}^{t}h_{i}　　　(3)$$\n",
    "\n",
    "**上下文向量**，其被视为当前时间步从编码器的源输入中所读取的一个固定尺寸的表示，与解码器的状态$s_{t}$拼接在一起，然后通过两个线性层产生词汇分布 $P_{vocab}$:\n",
    "\n",
    "$$P_{vocab} = softmax\\left(V^{'}\\left(V\\left[s_{t},h_{t}^{*}\\right] + b\\right) + b^{'}\\right)　　　(4)$$\n",
    "\n",
    "这里 $V,V^{'},b,b^{'}$ 是可学习的参数。$P_{vocab}$是在词包中所有词上的概率分布，为我们提供了预测单词 $w$的最终分布：\n",
    "\n",
    "$$P(w) = P_{vocab}(w)　　　(5)$$\n",
    "\n",
    "训练阶段，时间步$t$的损失是目标单词$w_{t}^{*}$的负的对数似然：\n",
    "\n",
    "$$loss_{t} = -logP(w_{t}^{*})　　(6)$$\n",
    "\n",
    "整个序列的总损失是:\n",
    "\n",
    "$$loss = \\frac{1}{T}\\sum_{t=0}^{T}loss_{t}　　　(7)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指针-生成 网络：Pointer-generator network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 覆盖机制：Coverage mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相关工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经生成式摘要：Neural abstractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指针-生成 网络：Pointer-generator networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论：Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
