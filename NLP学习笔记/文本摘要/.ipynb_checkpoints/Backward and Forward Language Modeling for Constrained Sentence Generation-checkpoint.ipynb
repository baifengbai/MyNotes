{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文学习于论文《Backward and Forward Language Modeling for Constrained Sentence Generation》，作者: Lili Mou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要: Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最近的语言模型，尤其是基于循环神经网络RNN的语言模型，使得从学到的概率中生成自然语言成为可能。语言生成具有广泛的应用，包括机器翻译、摘要、问答、会话系统等。现有的方法通常学习以额外信息为条件的单词的联合概率，该额外信息(静态或动态地)馈送到RNN的隐藏层。在许多应用中，我们可能对生成的文本施加严格的约束，即，必须在句子中出现特定的单词。不幸地是，现有的方法仍无法解决这个问题。在本文中，我们提出了一种新颖的后向和前向语言模型。给定特定单词，我们使用RNN同时或异步地生成先前的单词和将来的单词(generate previous words and future words),从而产生了两个模型变体。通过这种方式，给定的单词可以出现在句子的任何位置。实验结果表明，生成的文本与序列语言模型的结果相当。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引言: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "语言建模旨在最小化语料库的联合概率。它一直是自然语言处理的核心(D. Jurafsky的《Speech and Language Processing》)，同时启发了多种模型，例如n-gram模型，平滑技术，以及NLP中的多种神经网络。特别是，神经模型的重新繁荣已经在许多任务中取得了突破性进展，包括语言建模本身(Y. Bengio《A neural probabilistic language model》)，词性标注，命名实体识别和语义角色标注。\n",
    "\n",
    "循环神经网络RNN是一种流行的语言模型；它适合于通过其迭代性质对时间序列数据(例如，单词序列)进行建模。RNN通常会保留一个或几个隐藏层；在每个时隙，它读取一个单词，并相应地改变它自身的状态。与传统的n-gram模型相比，RNN更能够学习远距离特征--尤其是LSTM(S. Hochreiter的《Long short-term memory》)或GRU(Y. Bengio的《On the properties of neural machine translation: Encoder-decoder approaches》)--因此，它可以更好地捕获句子的本质。在此基础上，甚至可以从RNN语言模型生成句子，该模型在NLP中具有广泛的应用，包括机器翻译、生成式文本摘要、问答和会话系统。句子生成过程通常通过每次选择最可能的单词来完成，这个最可能的单词取决于前面的单词以及依赖于任务的附加信息(例如，在机器翻译系统中源句子的向量表示)。\n",
    "\n",
    "然而，在许多情况下，我们可能会对生成的句子施加约束。例如，问答系统可能涉及分析问题和查询现有知识库，到了这个地步，一个候选答案就在眼前。然后，自然语言生成器应该生成一个句子，它在语义上连贯同时包含候选答案。不幸的是，使用现有的语言模型来生成一个具有给定单词的句子是非凡的:添加单词的其他信息并不能保证所需的单词会出现；一般的概率采样器几乎不适用于RNN语言模型；将任意单词设置为想要的单词会损害句子的流畅性；对第一个单词施加约束限制了生成句子的形式。\n",
    "\n",
    "在本文中，我们提出了一种新颖的后向和前向(B/F)语言模型来解决受约束的自然语言生成问题。**我们使用RNN生成以给定单词为条件的先前和后续单词，而不是像传统模型那样依次生成从第一个单词到最后一个单词的句子**。前向和后向生成可以同步或异步完成，从而产生两种变体:syn-B/F,asyn-B/F。通过这种方式，我们的模型在理论上是完整的，用于生成具有所需单词的句子，该单词可以出现在句子中的任意位置。\n",
    "\n",
    "论文的其余部分组织如下：第二部分回顾存在的语言模型和自然语言生成器；第三部分描述我们提出的反向和正向语言模型；第四部分展示实验结果；第五部分，得出结论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 背景和相关研究: Background and Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，这里$\\omega$用来指代**单词序列**，$w$表示**某个单词**。$x,h$都是向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语言建模: Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定语料 $\\omega = w_{1},...,w_{m}$ ,语言建模旨在最小化 $\\omega$ 的联合分布，即 $p(\\omega)$ 。受到人们总是从头到尾说句子的启发，我们将联合概率分布分解为(Y. Bengio《A neural probabilistic language model》):\n",
    "\n",
    "$$p(\\omega) = \\prod_{t=1}^{m}p(w_{t}|\\omega_{1}^{t-1})　　　(1)$$\n",
    "\n",
    "$w_{1},w_{2},...,w_{t}$ 用 $w_{1}^{t}$简化表示。\n",
    "\n",
    "通过多项分布进行参数化，我们需要进一步简化上述方程以估计参数。加上马尔科夫假设--一个词只依赖于先前的 $n-1$ 个单词并且与其位置无关---结果便是经典的 n-gram模型，其中的联合概率由下式给出:\n",
    "\n",
    "$$p(\\omega) \\approx \\prod_{t=1}^{m} p(w_{t}|\\omega_{t-n+1}^{t-1})　　　(2)$$\n",
    "\n",
    "为了减轻数据稀疏性问题，已经提出了很多平滑方法。我们为对n-gram模型及其变体感兴趣的读者推荐(D. Jurafsky and J. H. Martin《Speech and Language Processing》)这样的教科书。\n",
    "\n",
    "Bengio等人(《A neural probabilistic language model》)提出用前馈神经网络来估计等式(2)中的概率。在他们的模型中，单词首先映射到一个低纬的向量，如embedding；然后，一个前馈神经网络将信息传播到softmax输出层，该层用来估计下一个单词的概率。\n",
    "\n",
    "RNN模型也可以用于语言建模。该模型保留了$t$时刻的隐藏状态向量 $h_{t}$，该隐藏状态向量依赖于前一时刻的隐藏状态向量 $h_{t-1}$ 和当前的输入向量 $x$(该向量是当前输入单词的嵌入表示)。输出层用来估计每个单词在此时刻出现的概率。下面列出了普通RNN的公式:\n",
    "\n",
    "$$h_{t} = RNN(x_{t},h_{t-1})　　　　(3)$$\n",
    "\n",
    "$$= f(W_{in}x_{t} + W_{hid}h_{t-1}) 　　　(4)$$\n",
    "\n",
    "$$p(w_{t}|\\omega_{0}^{t-1}) \\approx softmax(W_{out}h_{t})　　　(5)$$\n",
    "\n",
    "如公式所示，RNN提供了等式(1)的直接参数化方法，因此与n-gram模型相比具有捕获长期依赖性的能力。实践中，普通的RNN由于梯度弥散或梯度爆炸问题很难训练；LSTM 和 GRU 被提出来更好地平衡先前的状态和当前的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语言生成:  Language Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用RNN来建模语言的联合概率使得生成新句子称为可能。早期尝试通过字符级RNN语言模型生成文本(I. Sutskever《Generating text with recurrent neural networks》)；最近，基于RNN的语言生成在几个实际应用中取得了突破。\n",
    "\n",
    "序列到序列的机器翻译模型使用RNN将源句子编码成一个或几个固定大小的向量；然后，另一个RNN将向量解码成目标句子(I. Sutskever《Sequence to sequence learning with neural networks》)。这种网络可以被视为以源句子为约束的语言模型。在每个时间步，RNN预测最可能的单词作为输出；下一步，将单词的嵌入表示馈送到输入层。这个过程会持续下去，直到RNN生成一个特殊的符号$<EOS>$,它表明解码到了句子结尾。Beam search(I. Sutskever《Sequence to sequence learning with neural networks》) 和 抽样方法(T.-H. Wen《Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking》)可以用来提高生成文本的质量和多样性。\n",
    "\n",
    "如果源句子太长而无法放入一个或几个固定大小的向量中，则可以使用注意力机制在目标生成期间动态关注源句子的不同部分(D. Bahdanau《Neural machine translation by jointly learning to align and translate》)。在其它研究中，Wen等人使用RNN基于语义的一些抽象表示来生成句子；他们将一个one-hot向量作为附加信息提供给RNN的隐藏层(T.-H. Wen《Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking》)。在问答系统中，Yin等人利用soft logistic switcher(softmax)从词汇表中生成单词或复制候选答案(J. Yin《 Neural generative question answering》)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B/F语言模型: The Proposed B/F Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这部分，我们会详细地介绍我们的B/F语言模型。我们的直观认识是寻求一种新方法来分解句子的联合概率(等式(1))。如果我们事先知道一个单词 $w_{s}$ 应该出现在句子中($\\omega = w_{1},...,w_{m},w_{s} \\in \\omega$),那么很自然地设计出一个贝叶斯网络，其中 $w_{s}$ 是根节点，其它单词以 $w_{s}$ 为条件。遵循序列生成的思想， $w_{s}$ 将句子分成两个子序列:\n",
    "\n",
    "1. 反向序列: $w_{s},w_{s-1},w_{s-2},...,w_{1}$ (s 个单词)\n",
    "2. 前向序列: $w_{s},w_{s+1},w_{s+2},...,w_{m}$ (m-s+1 个单词)\n",
    "\n",
    "在位置 s 处分割的序列$\\omega$的概率分解如下:\n",
    "\n",
    "$$p\\left(\\frac{\\underline{\\omega_{s}^{1}}}{\\omega_{s}^{m}}\\right) = p(w_{s})\\prod_{i=0}^{s}p^{(bw)}\\left(w_{s-i}|·\\right)\\prod_{i=0}^{m-s+1}p^{(fw)}\\left(w_{s+1}|·\\right)　　　(6)$$\n",
    "\n",
    "$p(\\frac{\\underline{·}}{·})$ 表示特定后向/前向序列的概率\n",
    "\n",
    "为了对等式进行参数化，我们提出了两种模型变体。第一种方法是同时生成前向和后向模型，我们称之为 syn-B/F 语言模型，如下图所示:\n",
    "![](./image/BF_figure_1.png)\n",
    "\n",
    "具体而言，等式(6)采用如下形式:\n",
    "\n",
    "$$p\\left(\\frac{\\underline{\\omega_{s}^{1}}}{\\omega_{s}^{m}}\\right) = \\prod_{t=0}^{max\\left\\{s,m-s+1\\right\\}-1} p\\left(\\frac{\\underline{w_{s-t}}}{w_{s+t}}\\mid\\frac{\\underline{\\omega_{s}^{s-t+1}}}{\\omega_{s}^{s+t-1}}\\right)　　　(7)$$\n",
    "\n",
    "其中，因子$p(=|=)$ 是指当前时间步$t$在前向序列和反向序列中生成$w_{s-t},w_{s+t}$的条件概率，分别给出句子的中间部分，即 $w_{s-t+1},...,w_{s},...,w_{s+t-1}$, $t$从0开始。如果前向或后向中的某个序列生成了$<EOS>$,我们会为此序列填充特殊符号EOS,直到另一序列也生成$<EOS>$为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据第二节介绍的研究，我们还利用了RNN。等式(7)中的因子由下式计算:\n",
    "\n",
    "$$p\\left(\\frac{\\underline{w_{s-t}}}{w_{s+t}}|\\frac{\\underline{\\omega_{s}^{s-t+1}}}{\\omega_{s}^{s+t-1}}\\right)　　　(8)$$\n",
    "$$= p^{(bw)}(w_{s-t}|h_{t})·p^{(fw)}(w_{s+t}|h_{t})　　　(9)$$\n",
    "$$= softmax\\left(W_{out}^{(bw)}h_{t}\\right)·softmax\\left(W_{out}^{(fw)}h_{t}\\right)　　　(10)$$\n",
    "\n",
    "这里，$h_{t}$是隐藏层，它依赖于先前状态$h_{t-1}$和当前输入单词的嵌入 $\\widetilde{x} = \\left[x_{t}^{(fw)};x_{t}^{(bw)}\\right]$ 。在模型中我们使用GRU(K. Cho《On the properties of neural machine translation: Encoder-decoder approaches》)，给定如下公式:\n",
    "\n",
    "$$r = \\sigma\\left(W_{r}\\widetilde{x} + U_{r}h_{t-1}\\right)　　　(11)$$\n",
    "\n",
    "$$z = \\sigma\\left(W_{z}\\widetilde{x} + U_{z}h_{t-1}\\right)　　　(12)$$\n",
    "\n",
    "$$\\widetilde{h} = tanh\\left(W_{x}\\widetilde{x} + U_{x}(r\\circ h_{t-1})\\right)　　　(13)$$\n",
    "\n",
    "$$h_{t} = (1-z)\\circ h_{t-1} + z \\circ \\widetilde{h}　　　(14)$$\n",
    "\n",
    "这里$\\sigma(·) = \\frac{1}{1 + e^{(-·)}}$ ，$\\circ$ 指按元素计算的乘积。$r,z$ 是门，$\\widetilde{h}$是当前时间步的候选隐状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 同步-B/F模型中，我们设计了一个单独的RNN来生成两个链，希望每个链能意识到对方的状态。此外，我们还提出了一个 异步的版本，标记为 asyn-B/F,如下图所示:\n",
    "![](./image/BF_figure_2.png)\n",
    "\n",
    "这个模型的想法是，首先生成反向序列，然后将获得的结果馈送到另一个前向RNN中来生成 前向序列单词。详细的公式不再重复。\n",
    "\n",
    "重要的是要注意，用于反向序列生成的asyn-B/F的RNN与通用的反向语言模型不同。后者被假定为，从最后一个单词到第一个单词对句子建模，而我们我们的反向RNN，实际上是半个语言模型LM，从单词 $w_{s}$ 开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果总是假定 $w_{s}$ 给定，那么训练标准便是除了 $w_{s}$ 之外的两个链中所有单词的交叉熵损失。我们可以额外惩罚分割词 $w_{s}$ ，这将使得可以在没有提供 $w_{s}$ 的情况下生成整个句子。我们认为上述两个标准差异不大，在我们的实验中采用后者。\n",
    "\n",
    "标记和未标记的数据集都足以训练 B/F语言模型。如果一个句子用一个特别有趣的单词 $w_{s}$注释，那么将此单词用作分割词是很自然的。对于未标记数据集，我们可以随机选择一个单词作为分割词 $w_{s}$ 。\n",
    "\n",
    "注意到等式(6)给出了以单词 $w_{s}$ 为分割点的句子 $\\omega$ 的联合概率。为了计算句子的概率，我们将把不同的分割词求和，即:\n",
    "\n",
    "$$p(\\omega) = \\sum_{s=1}^{m} p\\left(\\frac{\\underline{w_{s}^{1}}}{w_{s}^{n}}\\right)　　　(15)$$\n",
    "\n",
    "然而，在我们的场景中，我们总是假定单词 $w_{s}$是给定的。因此，与一般的语言建模不同，句子的联合概率不是我们模型中关注的数字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集和设置: The Dataset and Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了评估B/F语言模型，我们更喜欢带有有趣应用程序块的垂直域语料库，而不是像维基百科那样使用通用文本。特别地，我们选择在arXiv上科学论文题目上建立语言模型。建立在论文题目上的语言模型可以帮助研究人员准备他们的论文草稿。提供了一个主题(由给定单词制定)，受约束的自然语言生成也可以作为头脑风暴的一种方式。\n",
    "\n",
    "我们爬取了2014年1月至2015年11月之间的与计算机科学相关的论文题目。每个单词都改成小写，但并没有去词干化。罕见单词(出现次数小于等于10)统一被归类为单个符号标记$<UNK>$,指代未知单词。我们删除了非英文标题，以及那些有三个以上$<UNK>$标记的标题。我们注意到$<UNK>$可能经常出现，但它们中有大量的是指缩写词，因此在语义上基本一致。\n",
    "\n",
    "目前，我们有25000个样本用于训练，1455个用于验证，另外1455个用于测试；我们的词汇量是3380。异步的B/F有一个含100个单元的隐藏层；同步的B/F则有200个；这是一个公平的比较，因为同步的B/F应该同时学习隐含的前向和后向语言模型，这和异步的是完全不同的。在我们的模型中，嵌入层是50个维度，随机初始化。为了训练模型，我们使用标准的反向传播(batch大小为50)和逐元素梯度裁剪。像A. Karpathy的论文《Visualizing and understanding recurrent networks》一样，我们应用rmsprop进行优化(排除嵌入)，它与朴素的随机梯度下降相比，更适合训练RNN，与动量方法相比，对超参数不敏感。最初的权重是从$[-0.08,0.08]$ 中均匀采样的。初始的学习率为0.002，学习率衰减为0.97，移动平均值衰减为0.99，阻尼项$\\epsilon = 10^{-8}$ 。由于词嵌入在使用中是稀疏的，它们通过普通的随机梯度下降异步优化，此时学习率除以$\\sqrt{\\epsilon}$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果: Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文中年，我们为受约束的自然语言生成模型提出了一个后向和前向的语言模型B/F。给定一个词，我们的模型可以同步或异步地生成该词前面的单词和后面的单词。如果我们忽略随机分裂引入的复杂度，实验表明我们的模型与序列语言模型有类似的复杂度。我们的案例研究表明，异步的B/F语言模型可以生成包含给定单词的句子，并且在质量上与序列语言模型相当。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
