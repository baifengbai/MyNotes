{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文学习于论文[Recurrent Convolutional Neural Networks for Text Classification](http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552).\n",
    "\n",
    "编辑:Weiyang,Time:2019.2.21,公众号:FinTech修行僧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摘要:Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本分类是许多NLP应用的基础任务。传统的文本分类器往往依赖于许多人为设计的特征，如 字典、知识库和特殊的**树核**。\n",
    "\n",
    "相比于传统文本分类方法，我们引入了一种用于文本分类的**循环卷积神经网络(RCNN)**，它没有任何人为设计的特征。\n",
    "\n",
    "RCNN在学习**词汇的表示**(word representations)时，用一个**循环结构**(recurrent structure)来捕捉尽可能远的**上下文信息**(contextual information),与传统**基于窗口的神经网络**相比，它可能引入了相当少的噪声。\n",
    "\n",
    "RCNN也使用了**最大池化层(max-pooling)**以**捕获文本中的关键部分**，它可以自动判断哪个词在分类中是重要的。\n",
    "\n",
    "我们在四个常用的数据集上进行了实验，结果表明RCNN在数据集上的表现要优于最先进的技术，尤其在文档级数据集上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引言: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词袋模型BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本分类是许多应用中的重要成分，例如网页搜索、信息过滤和情感分析。因此，它吸引了大量研究者的注意。\n",
    "\n",
    "文本分类中一个关键问题是**特征表示**，其普遍基于词袋模型(BoW)，其中unigrams、bigrams、n-grams以及一些精心设计的模式常作为特征抽取。更深一步，许多特征选择方法，例如词频,MI,pLSA,LDA被用于选择有区分度的特征。\n",
    "\n",
    "然而，传统的特征表示方法常忽视语境信息或单词顺序，其捕获单词语义的能力仍不能令人满意。例如，在句子\"A sunset stroll along the South Bank affords an array of stunning vantage points.\"中，当我们分析单词\"Bank\"(unigram)时，我们也许并不知道它指代的是金融机构还是河边上的陆地。\n",
    "\n",
    "此外，短语\"South Bank\"(bigram)，特别是考虑到两个大写字母，也许会误导那些对伦敦并不熟悉的人，因为他们不会把它当做金融机构。在我们获得了一个更大的语境\"stroll along the South Bank\"(5-gram)后,我们很容易区分它的含义。\n",
    "\n",
    "尽管更高阶的n-grams和更复杂的特征被设计来捕获更多语境信息和单词顺序，但是仍然面临着数据稀疏性的问题，而这严重地影响了分类的准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词的分布式表示: word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "近来，预训练词嵌入和深度神经网络的快速发展为许多NLP任务带来了鼓舞。词嵌入(word embedding)是词的分布式表示，大量缓解了数据稀疏性问题。Mikolov, Yih, and Zweig展示了预训练词嵌入能够捕获有意义的语法和语义规律。有词嵌入的帮助，一些组合的方法被提出来捕获文本的表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RecursiveNN的优缺点: 树结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Socher et al.提出了递归神经网络RecursiveNN,就构建句子表示方面它被证明是有效的。然而，**RecursiveNN通过一个树结构来捕获句子的语义**。它的表现非常依赖于文本树构造的表现。进一步而言，构造这样的文本树的时间复杂度至少为$O(n^{2})$,$n$是文本的长度。当遇到长句子或文档时，这会消耗大量的时间。此外，两个句子的关系很难被树结构表示。因此，**RNN不适合对长句子或文档进行建模**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN的优缺点： 有偏的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一个模型:循环神经网络RNN(RecurrentNN),时间复杂度只有$O(n)$。**RNN通过一个词接着一个词来分析文本，并通过固定尺寸的隐藏层来储存先前所有的文本**。RNN的优势是能更好的捕获上下文信息，这有助于捕获长文本的语义。然而，RNN是一个**有偏的模型**，因为文本的单词序列中后面的单词比前面的单词更占优势。因此，当捕获全文档的语义时，RNN的有效性会削弱，因为**关键的文本成分可以出现在文档的任何部分，而不只是末尾**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN的优缺点： 无偏的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了处理**RNN的有偏问题**，**卷积神经网络CNN，一个无偏的模型**，被引进了NLP任务中，**它通过一个最大池化层来公平地确定文本中有区分度的短语**。因此，相比于递归神经网络和RNN，CNN也许会更好地捕获文本的语义。CNN的时间复杂度也是$O(n)$。然而，先前对CNN的研究倾向于使用简单的卷积核，例如固定的窗口。当使用这些卷积核时，很难决定窗口的尺寸:**小的窗口也许会导致丢失关键信息，而大的窗口会导致极大的参数空间，这使得CNN难训练**。所以，便有了这个问题:<font color=red>我们能不能比传统的基于窗口的网络学到更多的上下文信息，并且更准确地表示文本的语义用于文本分类。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RCNN的提出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了解决上述模型的缺点限制，我们提出了 Recurrent Convolutional Neural Network(RCNN),并把它用于文本分类任务。首先，我们使用一个双向循环结构，它相比于传统的基于窗口的模型噪声更少，在学习单词表示时尽可能地捕获上下文信息。此外，当学习文本表示时，RCNN能够保留更大范围的单词顺序。其次，我们使用一个最大池化层来自动判别在文本分类中起重要角色的特征，以捕获文本中的关键成分。最后，我们的模型的复杂度为$O(n)$,它只与文本长度的长度线性相关。\n",
    "\n",
    "我们将RCNN 与最先进的技术，在四个不同的英文和中文数据集上做了对比。分类的主题包括:主题分类、情感分类和写作风格分类。实验表明，RCNN在其中3个数据集上的表现比最先进的技术更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相关的研究：Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本分类: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传统的文本分类工作主要关注三个主题:**特征工程**(feature engineering)、**特征选择**(feature selection)和**机器学习算法**(machine learning algorithm)。\n",
    "\n",
    "在特征工程中，被广泛使用的特征是**词袋特征**(bag-of-words).此外，也有一些复杂的特征，比如**词性标签**(part-of-speech tags),**名词短语**(noun phrases) 和**树核**(tree kernels).\n",
    "\n",
    "特征选择旨在**剔除噪声特征**，提升分类的性能。最广泛使用的特征选择方法是**移除停用词**(removing the stop words)。更先进的技术是使用**信息增益**(information gain),**互信息**(mutual information)和**$L1$正则**。\n",
    "\n",
    "机器学习算法往往使用**逻辑回归**(LR)，**朴素贝叶斯分类**(NB)和**支持向量机**(SVM)。然而，这些方法都面临着**数据稀疏性问题**(data sparsity problem)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度神经网络: Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "近来，深度神经网络和**表示学习**(representation learning)为解决数据稀疏性问题(data sparsity problem)带来了新的想法，出现了许多学习词表示(word representations)的神经模型。\n",
    "\n",
    "一个词的神经表示(the neural representation of word)称为**词嵌入**(word embedding)，它是一个实值向量(real-valued vector)。词嵌入使得我们能够通过简单地使用两个嵌入向量之间的距离来度量**单词之间的相关性**(relatedness)。\n",
    "\n",
    "使用**预训练的词嵌入**(pre-trained word embeddings)，使得神经网络在许多NLP任务上表现出色。例如,Socher et al.使用半监督的递归自编码器来预测句子的情感;Socher et al.提出了一种短语检测的循环神经网络方法;Socher et al.提出了递归神经张量网络来分析短语和句子的情感;Mikolov使用循环神经网络来构建语言模型。Kalchbrenner和Blunsom提出了一种新的对话行为分类的循环神经网络;Collobert et al.提出了语义角色标注的卷积神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RCNN模型: Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们提出了一个深度神经网络模型(RCNN)来捕获文档的语义。图1展示了RCNN的网络结构:\n",
    "![RCNN](./image/RCNN.png)\n",
    "\n",
    "1. 左侧一系列垂直向下的箭头，表示**正向RNN的传递** \n",
    "2. 右侧一系列垂直向上的箭头，表示**逆向RNN的传递** \n",
    "3. 斜向下的箭头表示箭头起始处的单词参与箭头终点处的单词的**左侧上下文计算**\n",
    "4. 斜向上的箭头表示箭头起始处的单词参与箭头终点处的单词的**右侧上下文计算**\n",
    "5. $y^{(i)}$表示网络的第几层，RCNN共有四层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RCNN的输入是文档$D$,它是一个单词序列:$w_{1},w_{2},...,w_{n}$;输出是各个类别的概率 。 我们用$p(k|D,\\theta)$指代文档$D$属于类别$k$的概率，$\\theta$是RCNN的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词汇表示的学习: Word Representation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单词上下文的学习: 左侧上下文 和 右侧上下文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将**单词**和**单词的上下文**(context)<font color=red>拼接</font>在一起来**表示这个单词**。上下文(context)帮助我们获取更加精准的单词含义(word meaning)。在我们的模型中，我们**使用了一个循环结构来捕获上下文信息**，这个循环结构是一个双向循环神经网络(bi-directional recurrent neural network)。\n",
    "\n",
    "我们用$c_{l}\\left(w_{i}\\right)$表示单词$w_{i}$左侧的上下文信息，用$c_{r}\\left(w_{i}\\right)$表示单词$w_{i}$右侧的上下文信息。$c_{l}\\left(w_{i}\\right)$和$c_{r}\\left(w_{i}\\right)$都是维度为$|c|$的实值稠密向量。\n",
    "\n",
    "**正向RNN**的计算，$c_{l}\\left(w_{i}\\right)$由如下公式计算:\n",
    "$$c_{l}\\left(w_{i}\\right) = f\\left(W^{(l)}c_{l}\\left(w_{i-1} \\right) + W^{(sl)}e\\left(w_{i-1} \\right) \\right)  (1)$$\n",
    "\n",
    "**逆向RNN**的计算，$c_{r}\\left(w_{i}\\right)$由如下公式计算:\n",
    "$$c_{r}\\left(w_{i}\\right) = f\\left(W^{(r)}c_{r}\\left(w_{i+1} \\right) + W^{(sr)}e\\left(w_{i+1} \\right) \\right)  (2)$$\n",
    "\n",
    "$e\\left(w_{i-1} \\right)$是单词$w_{i-1}$的词嵌入表示，$e\\left(w_{i+1} \\right)$是单词$w_{i+1}$的词嵌入表示，它们都是维度为$|e|$的实值稠密向量。\n",
    "\n",
    "文档中第一个单词是没有左侧上下文信息的，但我们给它赋予一个左侧上下文，用$c_{l}(w_{1})$表示;文档中最后一个单词是没有右侧上下文信息的，但我们给它赋予一个右侧上下文，用$c_{r}(w_{n})$表示;$c_{l}(w_{1})$和$c_{r}(w_{n})$将会赋予一个初始值，通过训练再进一步学习其值，且**训练数据中任何一篇文档都共享这两个值**。也就是说，如果每条训练数据是一个句子，则所有句子都共享这两个值;如果每条训练数据是一篇文档，则每篇文档共享这两个值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 双向RNN的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>正向RNN的参数介绍: 用来生成单词的左侧上下文向量</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W^{(l)}$是一个将隐藏层(其实是一个上下文context)转换到下一个隐藏层的矩阵，即网络中的参数矩阵；$W^{(sl)}$是用来将上一个单词($w_{i-1}$)的语义(word embedding)与这个单词($w_{i-1}$)的左侧上下文拼接在一起的矩阵，也是一个网络中的参数矩阵;$f$是一个非线性的激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>逆向RNN的参数介绍: 用来生成单词的右侧上下文向量</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W^{(r)}$是一个将隐藏层(其实是一个上下文context)转换到下一个隐藏层的矩阵，即网络中的参数矩阵；$W^{(sr)}$是用来将后一个单词($w_{i+1}$)的语义(word embedding)与这个单词($w_{i+1}$)的右侧上下文拼接在一起的矩阵，也是一个网络中的参数矩阵;$f$是一个非线性的激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 卷积层的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如同等式(1)和(2)展示的，上下文向量捕获了所有单词的左侧和右侧的上下文语义信息。例如，图1中，$c_{l}(w_{7})$编码了单词$Bank$左侧上下文信息**\"stroll along the South\"**,$c_{r}(w_{7})$则编码了其右侧上下文信息**\"affords an ...\"**.因此我们用公式(3)来定义**单词$w_{i}$的语义表示**(注意，不是词嵌入表示):\n",
    "\n",
    "$$x_{i} = \\left [c_{l}(w_{i});e(w_{i});c_{r}(w_{i}) \\right ] (3)$$\n",
    "\n",
    "显然单词$w_{i}$的语义表示,是其左侧上下文$c_{l}(w_{i})$，词嵌入表示$e(w_{i})$和其右侧上下文$c_{l}(w_{i})$的拼接(concatenation)。\n",
    "\n",
    "如此，使用这些上下文的信息，相比于使用**固定窗口的卷积神经网络**(它们只使用了文档的部分信息)，我们的模型可能会更好地**消除单词$w_{i}$含义的歧义**。\n",
    "\n",
    "循环结构(双向RNN)通过正向扫描文档能够获得所有的左侧上下文$c_{l}$,通过反向扫描文档能够获得所有的右侧上下文$c_{r}$，时间复杂度是$O(n)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>卷积层的作用</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们获得了每个单词$w_{i}$的语义表示$x_{i}$(RCNN第一层网络的结果)后,对每个$x_{i}$先用一个线性变换，再用一个$tanh$的激活函数，接着把结果$y_{i}^{(2)}$(RCNN第二层的结果)传递到第三层网络(最大池化层):\n",
    "\n",
    "$$y_{i}^{(2)} = tanh\\left(W^{(2)}x_{i} + b^{(2)} \\right) (4)$$\n",
    "\n",
    "$y_{i}^{(2)}$是一个潜语义向量，向量中每个语义因子(向量的每个维度值)被认为决定了代表文本的最有用的因子。\n",
    "\n",
    "每一个单词$w_{i}$的$x_{i}$都获得了一个$y_{i}^{(2)}$,相当于步长stride为1，$\\left[1 \\times N \\right]$的卷积核，，其中N为$x_{i}$的维度大小。**卷积层的作用便是获得每个单词代表文本重要性的潜语义向量$y_{i}^{(2)}$**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本表示的学习: Text Representation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最大池化层的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RCNN中的CNN的作用是用来表示文本。从CNN的角度看，先前的循环结构(双向RNN)便是卷积层。\n",
    "\n",
    "当所有单词的潜语义向量$y_{i}^{(2)}$都计算完毕后，我们便对它应用**最大池化层**:\n",
    "\n",
    "$$y^{(3)} = max_{i=1}^{n} y_{i}^{(2)} (5)$$\n",
    "\n",
    "$max$函数是逐元素比较的函数，也就是说$y^{(3)}$的第$k$个元素(维度值)是所有$y_{i}^{(2)}$的第$k$个元素(维度值)的最大值。\n",
    "\n",
    "**最大池化层将可变长度的文本变为一个固定长度的向量。通过最大池化层，我们可以捕获整篇文档的信息。当然，还存在其它方式的池化层，比如，平均池化层。这里不采用平均池化层是因为只有少数单词和它们的组合对于捕获文档信息是有用的**。\n",
    "\n",
    "**最大池化层试图找到文本中最重要的潜语义因子**，它用循环结构(或卷积层)的输出作为输入，时间复杂度是$O(n)$。RCNN是循环结构和最大池化层的级联，因此RCNN的复杂度依然是$O(n)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax概率输出层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RCNN的最后一层是输出层，类似于传统的神经网络，它被定义为:\n",
    "\n",
    "$$y^{(4)} = W^{(4)}y^{(3)} + b^{(4)} (6)$$\n",
    "\n",
    "最后，$softmax$函数被应用于$y^{(4)}$,以便将输出转为概率:\n",
    "\n",
    "$$p_{i} = \\frac{exp\\left(y_{i}^{(4)}\\right)}{\\sum_{k=1}^{n}exp\\left(y_{k}^{(4)} \\right)} (7)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RCNN四层网络结构总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 第一层:双向RNN，目的是产生每个单词的左侧上下文表示和右侧上下文表示\n",
    "2. 第二层:卷积层，目的是产生每个单词代表文本重要性的潜语义向量\n",
    "3. 第三层:最大池化层,目的是找到文本中最重要的潜语义因子\n",
    "4. 第四层:softmax概率输出层，目的是将第三层的结果通过一个线性变换转为概率输出\n",
    "\n",
    "第一层和第二层通常合并在一起称为循环结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练网络参数: Training Network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用$\\theta$来表示所有需要训练的参数:\n",
    "\n",
    "$$\\theta = \\left \\{E,b^{(2)},b^{(4)},c_{l}(w_{1}),c_{r}(w_{n}),W^{(2)},W^{(4)},W^{(l)},W^{(r)},W^{(sl)},W^{(sr)} \\right \\} (8)$$\n",
    "\n",
    "1. 参数$E \\in \\mathbb{R}^{|e| \\times |V|}$是词嵌入表示(word embeddings)\n",
    "2. $b^{(2)} \\in \\mathbb{R}^{H}$和$b^{(4)} \\in \\mathbb{R}^{O}$是偏置向量\n",
    "3. $c_{l}(w_{1}) \\in \\mathbb{R}^{|c|}$和$c_{r}(w_{n}) \\in \\mathbb{R}^{|c|}$是初始的上下文向量\n",
    "4. $W^{(2)} \\in \\mathbb{R}^{H \\times (|e|+2|c|)}$,$W^{(4)} \\in \\mathbb{R}^{O \\times H}$,$W^{(l)} $和$W^{(r)}$ $\\in \\mathbb{R}^{|c|\\times|c|}$,$W^{(sl)}$和$W^{(sr)}$$\\in \\mathbb{R}^{|e|\\times|c|}$,都是转换的参数矩阵\n",
    "5. 其中,$|V|$是词包的大小,$|H|$是隐藏层的结点数量,$|O|$是文档的类别数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RCNN的优化目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RCNN的训练目标是最大化参数$\\theta$的逻辑似然:\n",
    "\n",
    "$$\\theta \\mapsto \\sum_{D \\in \\mathbb{D}}logp\\left(class_{D}|D,\\theta \\right) (9)$$\n",
    "\n",
    "其中$\\mathbb{D}$是训练用的文档集，$class_{D}$是其相应的类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优化方法和 技巧(trick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用随机梯度下降法SGD来优化目标函数,在每一步中，都随机选取一个样本$(D,class_{D})$来更新梯度:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha\\frac{\\partial logp\\left(class_{D}|D,\\theta\\right)}{\\partial \\theta} (10)$$\n",
    "\n",
    "其中,$\\alpha$是学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在用SGD训练RCNN时，我们使用了一个广泛使用的技巧:**用均匀分布来初始化所有的参数**,**最大值或最小值的取值等于网络层扇入数量的平方根**。**扇入**是指前一层网络的结点数量。那一层的学习率将会用原始学习率除以扇入而得到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预训练的词嵌入: Pre-training Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词嵌入(word embeddings)是词的分布式表示，分布式表示非常适合作为神经网络的输入。传统的词表示，比如$one-hot$表示，会造成维度灾难。近来的研究表明，**使用无监督的预训练步骤后，神经网络可以收敛到更好的局部最小值**。\n",
    "\n",
    "在RCNN中，我们使用**Skip-gram**模型来预训练词向量(词嵌入)，**Skip-gram**模型是许多NLP任务中最先进的技术。\n",
    "\n",
    "**Skip-gram**模型通过最大化对数概率的平均值来训练词向量:$w_{1},w_{2},...,w_{T}$\n",
    "\n",
    "$$\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-c \\leq j \\leq c,j \\neq 0} logp(w_{t+j}|w_{t}) (11)$$\n",
    "\n",
    "$$p(w_{b}|w_{a}) = \\frac{exp\\left(e^{'}(w_{b})^{T}e(w_{a}) \\right)}{\\sum_{k=1}^{|V|}exp\\left(e^{'}(w_{k})^{T}e(w_{a})\\right)} (12)$$\n",
    "\n",
    "$|V|$是未标记文本的词包大小，$e^{'}(w_{i})$是另一个单词$w_{i}$的词嵌入表示。我们用$e$的嵌入表示，是由于运用了一些加速技巧,$e^{'}$实际中并没有计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验: Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集: Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了展示RCNN的性能，我们在如下四个数据集上做了实验，四个数据集详细情况如下:\n",
    "![dataset](./image/dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20Newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个数据集包含了20个新闻组的信息，我们使用了bydate版本(按日期)的数据集,并选取了其中四个主要的类别:comp,politics,rec,religion。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fudan set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个数据集是复旦大学文档分类数据集，有中文构成，包含了20个类别，包括:art,education,energy。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACL Anthology Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个数据集包含ACL和相关组织发布的科学文档。Post和Bergsma对文章作者的五种最常见的母语进行了标注:English,Japanese,German,Chinese,French。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stanford Sentiment Treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个数据集包含由Socher等人解析和标记的电影评论，标签为:Very Negative,Negative,Neutral,Positive,Very Positive 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验超参设置: Experiment Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词方法的选用和训练集、测试集的生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们是这样处理数据集的:\n",
    "\n",
    "对于英文文档，我们使用[Stanford Tokenizer](nlp.stanford.edu/software/tokenizer.shtml)来分词。对于中文文档，我们使用来[ICTCLAS](ictclas.nlpir.org)分词。我们不去除文本中的任何停用词或符号。所有的四个数据集都事先分割成训练集和测试集。ACL和SST数据集都有预先定义好的训练集，开发集和测试集。对于剩下的两个数据集，我们从训练集中抽取10%作为开发集，余下的90%仍为训练集。20Newsgroups数据集的评估指标是Macro-F1，其它三个数据集以准确率作为评估指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学习率、隐藏层、词向量大小、上下文向量大小的设置以及词向量的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RCNN的超参数设置依赖于所使用的数据集。我们在之前的研究上选取了一组常用的超参数集合。此外，我们设置随机梯度下降的学习率$\\alpha = 0.01$,隐藏层大小$H = 100$,词向量的大小$|e| = 50$,上下文向量的大小$|c| = 50$。我们用word2vec的默认参数来训练词向量，且模型使用Skip-gram，语料使用**Wikipedia dumps**英文和中文的词向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用于对比的其它文本分类方法: Comparision of Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将RCNN与其它广泛使用的文本分类方法以及最先进的技术在每个数据集上做了对比。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words/Bigrams + LR/SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wang 和 Manning提出了几个文本分类中强力的baseline。这些baselines主要使用unigram和bigram作为特征的机器学习算法。这里，我们分别使用逻辑回归和支持向量机算法。每个特征的权重是其词频。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Embedding + LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个方法使用带权重的单词的词嵌入(word embeddings)的平均数，其后用了softmax层来输出概率。每个单词的权重是其idf值。Huang et al.也将此策略作为全局语境用于他们的任务中。Klementiev, Titov, and Bhattarai在跨文档分类中也使用了这个策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于LDA的方法在捕获几个分类任务中的文本语义方面获得了良好的表现。我们选用了几个方法作为比较:ClassifyLDA-EM (Hingmire et al. 2013) and Labeled-LDA (Li, Sun, and Zhang 2008)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post and Bergsma 使用多种树核(tree kernels)作为特征,这是ACL传统文本分类中最先进的技术。我们列出了两种主要的方法作为比较:the context-free grammar (CFG) produced by the Berkeley parser (Petrov et al. 2006) and the reranking feature set of Charniak and Johnson (2005) (C&J)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RecursiveNN: 递归神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们选用了两种基于递归的方法作为比较: the Recursive Neural Network (RecursiveNN) (Socher et al. 2011a) and its improved version, the Recursive Neural Tensor Networks(RNTNs) (Socher et al. 2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们同时也选用了卷积神经网络(CNN)作对比。它的卷积核简单拼接预定义窗口内的词嵌入(word embeddings),形式如下:\n",
    "$$x_{i} = \\left [e(w_{i-\\left \\lfloor win/2 \\right \\rfloor});...;e(w_{i});...;e(w_{i+\\left \\lfloor win/2 \\right \\rfloor}) \\right ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RCNN和CNN捕获上下文信息的能力对比: Contextual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分我们会详细探讨RCNN中的循环结构捕获上下文信息的能力。\n",
    "\n",
    "**CNNs和RCNNs的区别是，它们使用不同结构来捕获上下文信息。CNNs使用一个单词的固定窗口作为上下文信息，而RCNNs使用了一个循环结构来捕获更广范围的上下文信息。CNN的表现受窗口的尺寸影响，小的窗口也许会导致一些长距离关系模式的丢失，而大的窗口将导致数据稀疏性。此外，大量的参数也会使得训练变得更加困难。**\n",
    "\n",
    "我们考虑了从1到19的所有奇数尺寸的窗口来训练和测试CNN模型。例如，当窗口尺寸为1时，CNN仅仅使用词嵌入$\\left [e(w_{i}) \\right ]$来表示单词$w_{i}$。当窗口尺寸为3时，CNN使用$\\left [e(w_{i-1});e(w_{i});e(w_{i+1}) \\right ]$来表示单词$w_{i}$。\n",
    "\n",
    "不同窗口大小的CNN的测试得分结果如下图所示:\n",
    "![rcnn_cnn](./image/rcnn_cnn.png)\n",
    "\n",
    "由于空间限制，我们只展示了在20Newsgroups的分类结果。在上图中，我们能够观察到RCNN的表现优于所有窗口尺寸的CNN。这表明，RCNN用循环结构能够捕获上下文信息而不依赖于窗口大小。\n",
    "\n",
    "RCNN的表现优于依赖于窗口的CNN，是因为循环结构能保留长上下文信息，且引进了很少的噪声。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们引进了RCNN来分类文本，RCNN用循环结构捕获了上下文信息，使用卷积网络构建了文本的表示。实验表明，在四个不同的数据集上，RCNN的表现比CNN和RecursiveNN要好。\n",
    "\n",
    "![RCNN_result](./image/RCNN_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据存储在./data/English_sentiment下,其中neg.txt存储的是负面的评论数据,pos.txt存储的是正面的评论数据,每行是一条评论数据;word_vectors.bin存储的是单词的词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取负面评论数据\n",
    "with open('./data/English_sentiment/neg.txt','r',encoding='utf-8-sig') as fi:\n",
    "    neg_text = fi.read()\n",
    "\n",
    "# 读取正面评论数据\n",
    "with open('./data/English_sentiment/pos.txt','r',encoding='utf-8-sig') as fi:\n",
    "    pos_text = fi.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据处理成模型可读的数据:\n",
    "1. 以'\\n'切割数据成句子列表,以空格切割单词\n",
    "2. 将英文单词全部转为小写\n",
    "3. 去除句中的特殊符号:;,.!?\"[]\\%等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "# 特殊符号\n",
    "symbols = string.punctuation\n",
    "neg_data = [] # 负面评论句子列表\n",
    "pos_data = [] # 正面评论句子列表\n",
    "vocabs = []   # 构造词包\n",
    "\n",
    "for sentence in neg_text.lower().split('\\n'):\n",
    "    words = []\n",
    "    for word in sentence.split():\n",
    "        word = re.sub(\"([{}])\".format(symbols),\"\",word)\n",
    "        if word == '':\n",
    "            continue\n",
    "        words.append(word)\n",
    "        vocabs.append(word)\n",
    "    neg_data.append(words)\n",
    "    \n",
    "for sentence in pos_text.lower().split('\\n'):\n",
    "    words = []\n",
    "    for word in sentence.split():\n",
    "        word = re.sub(\"([{}])\".format(symbols),\"\",word)\n",
    "        if word == '':\n",
    "            continue\n",
    "        words.append(word)\n",
    "        vocabs.append(word)\n",
    "    pos_data.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words of neg.txt and pos.txt is : 197290\n",
      "The length of vocabs is : 20516\n",
      "The number of neg.txt is : 5331\n",
      "The number of pos.txt is : 5331\n"
     ]
    }
   ],
   "source": [
    "print('The number of words of neg.txt and pos.txt is : %d'%len(vocabs))\n",
    "vocabs = list(set(vocabs)- set([''])) # 词包去重并去除空格字符\n",
    "print('The length of vocabs is : %d'%len(vocabs))\n",
    "print('The number of neg.txt is : %d'%len(neg_data))\n",
    "print('The number of pos.txt is : %d'%len(pos_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正负类比例为1:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将负面评论和正面评论合并在一起，并添加类别符号集合,其中:负面评论类别为0,正面评论类别为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = neg_data[:] + pos_data[:] # 全数据集\n",
    "labels = [0]*len(neg_data) + [1]*len(pos_data) # 类别集合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对单词编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_token = {word:token for token,word in enumerate(vocabs)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将数据中的英文替换为相应的数字编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2473, 3914, 8108, 11155],\n",
       " [13617,\n",
       "  20423,\n",
       "  11264,\n",
       "  8108,\n",
       "  7496,\n",
       "  12184,\n",
       "  66,\n",
       "  11449,\n",
       "  12687,\n",
       "  5509,\n",
       "  4461,\n",
       "  14565,\n",
       "  10830],\n",
       " [9471,\n",
       "  8108,\n",
       "  15377,\n",
       "  16406,\n",
       "  8331,\n",
       "  9757,\n",
       "  19120,\n",
       "  7302,\n",
       "  15915,\n",
       "  13558,\n",
       "  4205,\n",
       "  1273,\n",
       "  3902,\n",
       "  16708,\n",
       "  4607,\n",
       "  16027,\n",
       "  9361,\n",
       "  8331,\n",
       "  9757,\n",
       "  12814,\n",
       "  15215],\n",
       " [3986,\n",
       "  2144,\n",
       "  9757,\n",
       "  2482,\n",
       "  5125,\n",
       "  3211,\n",
       "  14468,\n",
       "  13489,\n",
       "  4919,\n",
       "  9757,\n",
       "  9813,\n",
       "  4935,\n",
       "  8331,\n",
       "  9757,\n",
       "  6253,\n",
       "  7210],\n",
       " [4607,\n",
       "  18175,\n",
       "  3370,\n",
       "  14734,\n",
       "  12403,\n",
       "  8946,\n",
       "  8108,\n",
       "  90,\n",
       "  2936,\n",
       "  2376,\n",
       "  16671,\n",
       "  16354,\n",
       "  8108,\n",
       "  15419],\n",
       " [9757,\n",
       "  11339,\n",
       "  13062,\n",
       "  11608,\n",
       "  11112,\n",
       "  3017,\n",
       "  11112,\n",
       "  8113,\n",
       "  10820,\n",
       "  4384,\n",
       "  13317,\n",
       "  980,\n",
       "  15914,\n",
       "  18846,\n",
       "  5102,\n",
       "  16646,\n",
       "  12426,\n",
       "  2733,\n",
       "  7047,\n",
       "  6341],\n",
       " [11771,\n",
       "  9757,\n",
       "  12184,\n",
       "  13543,\n",
       "  7047,\n",
       "  11512,\n",
       "  9757,\n",
       "  14916,\n",
       "  4691,\n",
       "  5125,\n",
       "  13062,\n",
       "  13198,\n",
       "  7047,\n",
       "  15098,\n",
       "  15712,\n",
       "  15025,\n",
       "  17938,\n",
       "  11719,\n",
       "  8108,\n",
       "  2957,\n",
       "  14565,\n",
       "  19357,\n",
       "  9757,\n",
       "  16471,\n",
       "  13331,\n",
       "  8810,\n",
       "  2896,\n",
       "  18846,\n",
       "  11479],\n",
       " [15580, 20423, 4830, 10381, 11112, 19919],\n",
       " [7268, 9757, 11339, 8108, 9757, 16247, 16745, 5925, 16970, 4607, 2157, 17341],\n",
       " [8577,\n",
       "  9757,\n",
       "  18846,\n",
       "  12175,\n",
       "  5125,\n",
       "  13617,\n",
       "  15983,\n",
       "  2768,\n",
       "  12597,\n",
       "  7047,\n",
       "  9757,\n",
       "  242,\n",
       "  18541,\n",
       "  14734,\n",
       "  14565,\n",
       "  6958,\n",
       "  10859,\n",
       "  16671,\n",
       "  13617,\n",
       "  4547,\n",
       "  7047,\n",
       "  12229,\n",
       "  13617,\n",
       "  2108]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for line in data:\n",
    "    new_line = []\n",
    "    for word in line:\n",
    "        token = word_to_token[word]\n",
    "        new_line.append(token)\n",
    "    temp.append(new_line)\n",
    "data = temp\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对数据集进行混洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "shuffle_indexs = np.random.permutation(range(0,len(labels))) # 混洗后的索引序列\n",
    "data = np.array(data)[shuffle_indexs]\n",
    "labels = np.array(labels)[shuffle_indexs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "class RCNN(t.nn.Module):\n",
    "    'RCNN实现'\n",
    "    def __init__(self,opt):\n",
    "        super(RCNN,self).__init__()\n",
    "        self.model_name = 'RCNN'\n",
    "        self.opt = opt\n",
    "        \n",
    "        kernel_size = opt.kernel_size\n",
    "        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim) # 嵌入层:(词包大小,嵌入维度)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = opt.embedding_dim, # 嵌入维度\n",
    "                                  hidden_size = opt.hidden_size,# 隐藏层单元数\n",
    "                                  num_layers = opt.num_layers,# 隐藏层数量\n",
    "                                  bias = True,# 加偏置\n",
    "                                  batch_first = False,# \n",
    "                                  dropout = 0.5,# 神经元随机休眠比例\n",
    "                                  bidirectional = True # 双向LSTM\n",
    "                                 )\n",
    "        self.conv = nn.Sequential(nn.Conv1d(in_channels = opt.hidden_size*2 + opt.embedding_dim,\n",
    "                                            out_channels = opt.dim,\n",
    "                                            kernel_size = kernel_size),\n",
    "                                   nn.BatchNorm1d(opt.dim),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Conv1d(in_channels = opt.dim,\n",
    "                                             out_channels = opt.dim, \n",
    "                                             kernel_size = kernel_size),\n",
    "                                   nn.BatchNorm1d(opt.dim),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.MaxPool1d(kernel_size = (opt.seq_len - kernel_size + 1))\n",
    "                                 )\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow实现"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
