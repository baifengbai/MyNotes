{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文学习于论文《Enriching Word Vectors with Subword Information》，作者:Tomas Mikolov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要: Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在大型未标记语料库上训练的连续单词表示，对于许多自然语言处理任务是有用的。学习这种表示的流行模型，通过为每个单词分配不同的向量来忽略单词的形态。这是一个限制，特别是对于具有大词汇量和许多罕见词汇的语言。在本文中，我们提出了一种基于 skip-gram模型的新方法，其中每个单词用一袋字符级的n-grams来表示。向量表示与每个字符级的n-gram相关联(字符级的n-gram，例如where，可表示为\"wh he er re\")；单词由这些字符级的n-gram的向量和表示。我们的方法很快，允许我们在大型语料库上快速训练模型，并且允许我们计算未出现在训练数据中的单词的表示。我们在九种不同的语言上评估我们的单词表示，包括单词相似度和类比任务。通过与最近提出的形态词表示进行比较，我们证明了我们的向量在这些任务上实现了最好的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引言: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习单词的连续表示在自然语言处理任务中有着悠久的历史(David E. Rumelhart《Neurocomputing: Foundations of research》chapter Learning Representations by Backpropagating Errors)。这些表示通常是由使用共现统计的大型未标记语料库派生而来。大量的分布式语义工作，已经研究了这些方法的属性。在神经网络社区，Ronan Collobert and Jason Weston《A unified architecture for natural language processing: Deep neural networks with multitask learning》提出使用前馈神经网络学习单词嵌入，其预测一个单词是基于左边两个单词和右边两个单词而实现的。最近，Ilya Sutskever《Distributed representations of words and phrases and their compositionality》提出用简单的对数双线性模型来有效地学习非常大的语料库上的单词的连续表示。\n",
    "\n",
    "这些技术中的大多数通过不同的向量来表示词汇表中的每个单词，而没有参数共享。特别是，它们忽略了单词的内部结构，这对于形态丰富的语言，如土耳其语或芬兰语是一个重要的限制。例如，在法语或西班牙语中，大多数动词有超过四十种不同的变形，而芬兰语中有十五种名词形式。这些语言包含许多在训练语料库中很少出现或根本不出现的单词形式，因此很难学习好的单词表示。由于许多单词构造都遵循一定的规则，因此可以通过使用字符级信息来改进形态丰富语言的向量表示。\n",
    "\n",
    "在本文中，我们建议学习字符级的n-grams的向量表示，并将单词表示为n-gram向量的和。我们的主要贡献是引入了一个连续skipgram模型的扩展，该模型考虑了单词的子词信息。我们在九种不同形态的语言上评估了这个模型，这显示了我们方法的优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相关工作: Related work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形态词表示: Morphological word representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "近年来，已经提出了许多方法来将形态学信息结合到单词表示中。为了更好地对罕见单词建模，Andrei Alexandrescu的《Factored neural language models》引入了分解神经语言模型，其中单词被表示为特征集。这些特征可能包括形态学信息，这种技术已成功应用于形态丰富的语言，例如土耳其语。最近，一些研究提出了不同的构词函数，以从语素中推导出单词的表示。这些不同的方法依赖于单词的语素分解，而我们的方法则不同。类似地，Chen等人《Joint learning of character and word embeddings》介绍了一种联合学习汉字词和字的嵌入表示的方法。Qing Cui等人《KNET: A general framework for learning word embedding using morphological knowledge》提出了限制形态相似的词应该有相似的表示。Radu Soricut 等人的《Unsupervised morphology induction using word embeddings》描述了一种学习形态转换的向量表示的方法，允许通过应用这些规则来获得未看到单词的表示。Ryan Cotterell的《Morphological word-embeddings》介绍了在经过形态学注释的数据上训练单词的表示法。与我们的方法最接近的是，Hinrich Schütze的《Word space》通过奇异值分解学习了字符级的4-gram的表示，并通过求和4-gram的表示来导出单词的表示。最近，John Wieting的《CHARAGRAM: Embedding words and sentences via character n-grams》还提出使用字符级的n-gram计数向量来表示单词。然而，用于学习这些表示的目标函数是基于释义对的，而我们的模型可以在任何文本语料库上训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP的字符级特征: Character level features for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与我们的工作密切相关的另一个研究领域是自然语言处理的字符级模型。这些模型摒弃了单词分割，旨在直接从字符中学习语言表达。这些模型的第一类是循环神经网络，其应用于语言建模，文本标准化，词性标注和解析。另一类模型是基于字符训练的卷积神经网络，用于词性标注、情感分析，文本分类和语言建模。Henning Sperr的《Letter n-gram-based input encoding for continuous space language models》介绍了一种基于受限玻尔兹曼机的语言模型，其中单词被编码为一组字符的n-gram集合。最后，最近的机器翻译工作Rico Sennrich的《Neural machine translation of rare words with subword units》提出了使用子词单位来获得罕见单词的表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型: Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通用模型: General Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 子词模型: Subword model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验设置: Expertimental setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基准: Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现细节: Implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集: Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果: Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定性分析: Qualitative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论: Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
