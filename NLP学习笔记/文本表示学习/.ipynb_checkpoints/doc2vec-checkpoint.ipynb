{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文学习于论文《Distributed Representations of Sentences and Documents》,作者:Tomas Mikolov 和 Quoc Le 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要: Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "许多机器学习算法要求输入表示为固定长度的特征向量。在文本方面，最常见的固定长度特征是词袋模型BOW。尽管它很受欢迎，但**词袋特征**有两个主要的缺点:它**丢失了单词顺序**和**忽略了单词的语义**。例如，\"powerful\"和\"strong\"，距离\"Paris\"同样遥远。在本文中，我们提出了**段落向量(Paragraph Vector)**,一种<font color=red>无监督算法</font>，**从可变长度的文本片段(如句子、段落和文档)中学习固定长度的特征表示**。我们的算法用**稠密向量表示每个文档**，该向量被训练来预测文档中的单词。该稠密向量的构造使我们的算法有可能克服词袋模型的弱点。实验结果表明，段落向量优于词袋模型以及其它文本表示技术。最后，我们在几个文本分类和情感分析任务中获得了最新的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引言: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本分类和聚类在许多应用程序中起着重要作用，例如文档检索、网页搜索、垃圾邮件过滤。这些应用程序的核心是机器学习算法，如逻辑回归或k-means。这些算法通常要求将文本输入表示为固定长度的向量。对于文本而言，最常见的**固定长度向量表示**可能是**词袋模型**或**n-gram模型**，因为它简单、高效且通常令人惊讶的准确率。\n",
    "\n",
    "然而，词袋模型有许多缺点。单词顺序丢失，因此不同的句子可以具有完全相同的表示，只要使用相同的单词即可。即使**n-gram模型**在短上下文语境中考虑了单词顺序，它也会受到**数据稀疏性**和**高维度**的影响。**词袋模型**和**n-gram模型**对**单词的语义**几乎没有意义，或者更正式地说**单词之间的距离**。这意味着单词\"powerful\",\"strong\"和\"Pairs\"距离相等，尽管事实在语义上，\"powerful\"应该比\"Pairs\"更接近\"strong\"。\n",
    "\n",
    "在本文中，我们提出了**Paragraph Vector**，一个无监督的框架，**用于学习文本片段(如句子，段落和文档)的连续分布式向量表示**。文本可以是可变长度的，范围从句子到文档。**Paragraph Vector**这个名称是为了强调，这个方法可用于**可变长度的文本**，从**短语**或**句子**到**大文档**的任何东西。\n",
    "\n",
    "在我们的模型中，向量表示被训练为用于预测段落中的单词。更准确地说，我们将**段落向量**与**段落中的几个单词向量**连接起来，并在给定上下文中预测下一个单词。单词向量和段落向量都是通过随机梯度下降和反向传播来训练的。虽然段落向量在段落中是唯一的，但是单词向量是共享的。在预测时，通过固定单词向量并训练新的段落向量直到收敛来推断段落向量。\n",
    "\n",
    "我们的技术受到最近使用神经网络学习单词向量表示的工作的启发。在他们的表述中，每个单词由一个向量表示，该向量与上下文中的其他单词向量连接或平均，并且结果向量用于预测上下文中的其他单词。例如，Bengio的论文《Neural probabilistic language models》提出的神经网络语言模型，使用几个先前单词向量的拼接来形成神经网络的输入，并试图预测下一个单词。结果是，在模型训练之后，将单词向量映射到向量空间，使得语义相似的单词具有相似的向量表示(例如，\"strong\"与\"powerful\"是很接近的)。\n",
    "\n",
    "根据这些成功的技术，研究人员已经尝试将模型扩展到超出单词级别，以实现短语级别或句子级别的表示。例如，一种简单的方法是使用文档中所有单词的加权平均值(Mitchell 的《Composition in distributional models of semantics》)。更复杂的方法是使用矩阵-向量运算以由句子解析树给出的顺序组合单词向量(Socher的《Dynamic pooling and unfolding recursive autoencoders for paraphrase detection》)。这两种方法都有弱点。第一种方法，即单词向量的加权平均，以与标准词袋模型相同的方式丢失了单词顺序。第二种方法，即使用解析树来组合单词向量，已被证明仅适用于句子，因为它依赖于解析。\n",
    "\n",
    "**段落向量Paragraph Vector**能够构造可变长度的输入序列的表示。与以前的一些方法不同，它是通用的，适用于任何长度的文本: 句子、段落和文档。它不需要特定任务的单词加权函数的调整，也不依赖于解析树。在本文中，我们将展示几个基准数据集的实验，这些数据集展示了段落向量Paragraph Vector的优势。例如，在情感分析任务中，我们获得了新的最先进的结果，比复杂的方法更好，在错误率方面产生了超过16%的相对改善。在文本分类任务中，我们的方法令人信服地击败了词袋模型，它相对改善30%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法: Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们首先讨论用于学习单词向量的先前方法。这些方法是我们段落向量**Paragraph Vector**方法的灵感来源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习单词的向量表示: Learning Vector Representation of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分介绍了单词的分布式向量表示。用于学习单词向量的广为人知的框架如下图所示，其任务是预测在上下文中给定其他单词的单词:\n",
    "\n",
    "![](./image/doc2vec_figure_1.png)\n",
    "\n",
    "在该框架中，每个单词被映射到一个唯一的向量，由矩阵$W$中的列表示。该列由词汇表中单词的位置索引。然后将上下文中给定单词向量的拼接或求和用作预测句子中的下一个单词的特征。\n",
    "\n",
    "更正式的是，给出一序列的训练单词$w_{1};w_{2};w_{3};...;w_{T}$,单词向量模型的训练目标是最大化平均对数概率:\n",
    "\n",
    "$$\\frac{1}{T}\\sum_{t=k}^{T-k}logp(w_{t}|w_{t-k},...,w_{t+k})$$\n",
    "\n",
    "预测任务通常是通过多分类器完成，例如$softmax$。因此，我们有:\n",
    "\n",
    "$$p(w_{t}|w_{t-k},...,w_{t+k}) = \\frac{e^{y_{w_{t}}}}{\\sum_{i}e^{y_{i}}}$$\n",
    "\n",
    "每个$y_{i}$是每个输出单词 $i$ 的非标准化对数概率，计算如下:\n",
    "\n",
    "$$y = b + Uh(w_{t-k},...,w_{t+k};W),(1)$$\n",
    "\n",
    "这里$U,h$ 是 $softmax$的参数，$h$ 是从矩阵$W$中抽取的单词向量的拼接或平均。\n",
    "\n",
    "在实践中，对于快速训练，分层$softmax$(Morin, Frederic and Bengio《Hierarchical probabilistic neural network language model》)优于$softmax$。在我们的工作中，分层$softmax$的结构是二进制霍夫曼树，其中短编码被分配给频繁的单词。这是一个很好的加速技巧，因为可以快速访问常用词。这种层次结构的二进制霍夫曼编码的使用与(Mikolov的《Distributed representations of phrases and their compositionality》)是相同的。\n",
    "\n",
    "基于神经网络的单词向量通常使用随机梯度下降来训练，其中梯度通过反向传播(Rumelhart《Learning representations by back-propagating errors》)获得。这种类型的模型通常称为神经语言模型(Bengio《Neural probabilistic language models》)。用于训练单词向量的基于神经网络的算法的特定实现可以在[以下处](code.google.com/p/word2vec/)获得。\n",
    "\n",
    "在训练收敛后，具有相似含义的单词被映射到向量空间中的类似位置。例如，单词'powerful'和'strong'彼此接近，而单词'powerful'和'Paris'则更为遥远。单词向量之间的差异也带有意义。例如，单词向量可用于使用简单向量代数回答类比问题:'King'- 'man' + 'woman' = 'Queen'。也可以学习线性矩阵来翻译语言之间的单词和短语(Mikolov的《Exploiting similarities among languages for machine translation》)。\n",
    "\n",
    "这些属性使单词向量对许多自然语言处理任务具有吸引力，例如 语言建模，自然语言理解，统计机器翻译，图像理解和关系抽取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 段落向量: 一个分布式记忆模型 | Paragraph Vector: A distributed memory model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的**Paragraph vector**向量方法受单词向量方法的启发。灵感是要求单词向量有助于关于句子中下一个单词的预测任务。因此，尽管单词向量是随机初始化的，但它们最终可以捕获语义作为预测任务的间接结果。我们将以类似的方式在段落向量中使用此想法。考虑到从段落中采样的许多上下文，段落向量也被要求对下一个单词的预测任务做出贡献。\n",
    "\n",
    "在我们的**Paragraph vector**向量框架(参见下图)中，每个段落都映射到一个唯一的向量，由矩阵 $D$ 中的列表示，每个单词也映射到一个唯一的向量，由矩阵 $W$ 中的列表示。段落向量和单词向量被平均或拼接以预测上下文中的下一个单词。在实验中，我们使用拼接作为组合向量的方法。\n",
    "\n",
    "![](./image/doc2vec_figure_2.png)\n",
    "\n",
    "更正式地说，与单词向量框架相比，该模型中唯一的变化是在等式(1)中，其中$h$由$W$和$D$构成。\n",
    "\n",
    "段落标记(Paragraph token)可以被认为是另一个单词。它充当了一个记忆，记住当前上下文中缺少的内容-或段落的主题。由于这个原因，我们称这个模型为 段落向量的分布式记忆模型 $PV-DM$。\n",
    "\n",
    "上下文是固定的，并从段落上的滑动窗口中采样。段落向量在从同一段落生成的所有上下文中共享，但不跨段落。然而，单词向量矩阵$W$在所有段落中是共享的，即单词'powerful'的向量在所有段落中是一样的。\n",
    "\n",
    "段落向量和单词向量通过随机梯度下降来训练，梯度通过反向传播来获得。在随机梯度下降的每个步骤中，可以从随机段落中采样固定长度的上下文，从图2中的网络计算误差梯度，并使用梯度更新模型中的参数。\n",
    "\n",
    "在预测时，需要执行推理步骤来计算新段落的段落向量。这也是通过梯度下降获得的。在该步骤中，模型的其余部分，单词向量矩阵$W$和$softmax$权重的参数是固定的。\n",
    "\n",
    "假设，语料中有$N$个段落，词汇表中有$M$个单词，我们想学习**Paragraph vector**向量，使每个段落映射到$p$个维度，每个单词映射到$q$个维度，因此模型总共有$N\\times p + M\\times q$个参数(包括softmax的参数)。尽管当$N$很大时，参数的数量可能很大，但是训练期间的更新通常是稀疏的并且因此是有效的。\n",
    "\n",
    "训练后，**Paragraph**向量可以被视为段落的特征。我们可以直接将这些特征放入传统的机器学习技术中，例如逻辑回归，支持向量机和k-means。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总之，算法本身分为两个阶段:\n",
    "1. 训练得到词向量矩阵$W$，softmax权重$U,b$和已见过的段落的**Paragraph vector**$D$;\n",
    "2. \"推理阶段\"获取新段落(从未见过)的**Paragraph vector**$D$。这是通过在$D$中添加更多列并保持$W,U,b$固定的同时在$D$上的梯度下降;\n",
    "\n",
    "我们使用D来对某些使用标准分类器的特定标签做预测，例如逻辑回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 段落向量的优势: Advantages of paragraph vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "段落向量的一个重要优点是它们从未标记的数据中学习，因此可以很好地处理没有足够标记数据的任务。段落向量还解决了词袋模型的一些关键弱点。首先，它们继承了单词向量的一个重要属性:单词的语义。在这个领域，'powerful'更接近'strong'而不是'Paris'。第二，它们至少在一个小的上下文中考虑了单词顺序，就像具有大$n$的n-gram模型一样。这很重要，因为n-gram模型保留了段落的很多信息，包括单词顺序。也就是说，我们的模型可能比bage-of-n-grams模型更好，因为bag-of-n-gram模型会创建一个非常高的维度表示，且往往泛化很差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不含单词顺序的段落向量: 分布式词袋 |Paragraph Vector without word ordering:Distributed bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述方法考虑用段落向量与单词向量的拼接来预测文本窗口中的下一个单词。另一种方法是忽略输入中的上下文单词，但强制模型预测从输出的段落中随机抽样的单词。实际上，这意味着在随机梯度下降的每次迭代中，我们对文本窗口进行采样，然后从文本窗口中采样随机词，并在给定**Paragraph vector**向量的情况下形成分类任务。这个技术由下图3表示。我们将此版本命名为**Paragraph vector**的分布式词袋版本**PV-DBOW**,而不是上一节中段落向量的分布式记忆版本**PV-DM**。\n",
    "![](./image/doc2vec_figure_3.png)\n",
    "\n",
    "除了概念上简单之外，该模型存储更少的数据。我们只需要存储softmax权重，而不是先前模型中的softmax权重和单词向量。这个模型与**word2vec**中的**skip-gram**模型很相似。\n",
    "\n",
    "在我们的实验中，每个**Paragraph vector**是两个向量的组合:一个用模型PV-DM学习而得，一个用模型PV-DBOW学习而得。单独的**PV-DM**通常适用于大多数任务(具有最好的性能)，但它与**PV-DBOW**的组合通常在我们尝试的许多任务中更加一致，因此强烈推荐。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相关研究: Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单词的分布式表示首先由(Rumelhart《Learning representations by back-propagating errors》)提出，并且已经成为一种成功的范例，特别是对于统计语言模型(Mikolov, Tomas.《 Statistical Language Models based on Neural Networks》)。词向量已经用于NLP应用，如单词表示，命名实体识别，词义消歧，句法解析，标记和机器翻译。\n",
    "\n",
    "短语表示是最近的趋势，并受到很多关注。在这个方向上，自动编码器式模型也被用于段落建模(Srivastava《Modeling documents with deep boltzmann machines》)。\n",
    "\n",
    "短语和句子的分布式表示也是(Socher的《Dynamic pooling and unfolding recursive autoencoders for paraphrase detection》)的焦点。他们的方法通常需要解析，结果显示适用于句子级表示，并且如何将他们的方法扩展到单个句子之外并不明显。他们的方法是监督型的，因此需要更多标记数据才能工作好。相反，**Paragraph vector**是无监督的，因此只需要很少的标记数据就能工作好。\n",
    "\n",
    "我们通过梯度下降计算段落向量的方法与计算机视觉中称为Fisher内核的成功范例相似。Fisher核的基本构造是无监督生成模型上的梯度向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们描述了**Parargraph vector**，这是一种无监督学习算法，可以学习可变长度文本(如句子和文档)的向量表示。向量表示通过预测从段落中采样的在上下文中的周围单词学习到。\n",
    "\n",
    "我们的实验在几个文本分类任务上，例如Stanford Treebank 和 IMDB情感分析数据集，表面该方法与最先进的技术相比具有竞争力。良好的性能证明了**Paragraph vector**在捕获段落语义方面的优点。事实上，**Paragraph vector**在克服词袋模型的弱点方面有潜力。\n",
    "\n",
    "虽然本论文的重点是表示文本，但我们的方法可以应用于学习序列数据的表示。在没有解析的非文本领域，我们希望**Paragraph vector**能够成为词袋模型和n-gram模型的强大替代品。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
