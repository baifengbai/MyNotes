{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文学习于:\n",
    "1. [贝叶斯优化(Bayesian Optimization)深入理解](https://www.cnblogs.com/marsggbo/p/9866764.html)\n",
    "2. [浅谈：高斯过程与贝叶斯优化](https://blog.csdn.net/a769096214/article/details/80920304)\n",
    "3. [强大而精致的机器学习调参方法：贝叶斯优化](https://www.cnblogs.com/yangruiGB2312/p/9374377.html)\n",
    "4. [贝叶斯优化 Bayesian Optimization](https://blog.csdn.net/Snail_Ren/article/details/79005069)\n",
    "5. [Bayesian Optimization Primer](https://sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf )\n",
    "6. [Cse.wustl.edu. Bayesian Optimization (2018)](https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf)\n",
    "7. [高斯过程](https://www.cnblogs.com/yifdu25/p/8092881.html)\n",
    "8. [浅谈高斯过程回归](http://www.cnblogs.com/hxsyl/p/5229746.html)\n",
    "9. [Gaussian process](https://en.wikipedia.org/wiki/Gaussian_process)\n",
    "10. [高斯过程回归简介学习笔记](http://blog.sina.com.cn/s/blog_5033f3b40102vts4.html)\n",
    "11. [高斯过程回归方法综述](https://xueshu.baidu.com/usercenter/paper/show?paperid=ac3bec8bf97a8da36b6cfa8e8f21aed8&tn=SE_baiduxueshu_c1gjeupa&ie=utf-8&site=baike)\n",
    "12. [机器学习有很多关于核函数的说法，核函数的定义和作用是什么？](https://www.zhihu.com/question/24627666?sort=created)\n",
    "13. [支持向量机](https://baike.baidu.com/item/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/9683835?fromtitle=SVM&fromid=4385807&fr=aladdin)\n",
    "14. [高斯过程](https://baike.baidu.com/item/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B)\n",
    "15. [核函数](https://baike.baidu.com/item/%E6%A0%B8%E5%87%BD%E6%95%B0)\n",
    "\n",
    "编辑:Weiyang,Time:2019.2.16,weixin:damo894127201"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n元正态分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一元正态分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果随机变量$X$的**概率密度函数**为\n",
    "$$f(x) = \\frac {1}{\\sqrt {2\\pi}\\sigma}e^{-\\frac {(x-\\mu)^{2}}{2\\sigma^{2}}} -\\infty < x < -\\infty$$\n",
    "\n",
    "则称$X$是服从参数为$\\mu$和$\\sigma^{2}$的正态分布的随机变量，简称为**正态随机变量**。该概率密度函数是一条关于$\\mu$对称的钟形曲线:\n",
    "![one_normal](./image/one_normal.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二元正态分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果二维随机变量$(X,Y)$的联合概率密度函数为\n",
    "\n",
    "$$ p(x,y) = \\frac {1}{2\\pi\\sigma_{1}\\sigma_{2}\\sqrt{1-\\rho^{2}}}exp\\left \\{-\\frac{1}{2(1-\\rho^{2})}\\left [\\frac{(x-\\mu_{1})^{2}}{\\sigma_{1}^{2}} - 2\\rho\\frac{(x-\\mu_{1})(y-\\mu_{2})}{\\sigma_{1}\\sigma_{2}} + \\frac{(y-\\mu_{2})^{2}}{\\sigma_{2}^{2}}\\right ] \\right \\},$$$$-\\infty < x,y < +\\infty$$\n",
    "\n",
    "则称$(X,Y)$服从**二元正态分布**，记为$(X,Y)\\sim N(\\mu_{1},\\mu_{2},\\sigma_{1}^{2},\\sigma_{2}^{2},\\rho)$,其中五个参数的取值范围分别是\n",
    "$$-\\infty < \\mu_{1},\\mu_{2} < +\\infty,\\sigma_{1},\\sigma_{2} > 0,-1 \\leq \\rho \\leq 1 $$ \n",
    "\n",
    "$\\mu_{1},\\mu_{2}$分别是$X$与$Y$的均值，$\\sigma_{1}^{2},\\sigma_{2}^{2}$分别是$X$与$Y$的方差，$\\rho$是$X$与$Y$的相关系数。\n",
    "\n",
    "二元正态分布的概率密度函数的图形很像一顶四周无限延伸的草帽，其中心点在$(\\mu_{1},\\mu_{2})$处，其等高线是椭圆:\n",
    "![two_normal](./image/two_normal.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二元正态分布满足以下两个性质(推广到n元正态分布也有类似的性质):\n",
    "1. 二元正态分布的边缘分布(边际分布)为一元正态分布，且二元正态分布的边际分布中不含相关系数$\\rho$\n",
    "2. 二元正态分布中除含有各分量的边际分布外，还含有两个分量间相互关联的信息，描述这种相互关联程度的一个特征数就是协方差\n",
    "\n",
    "从上面两条信息可得:已知n元联合正态分布，推导其变量的边际分布很容易；已知各个变量边际分布，推导n元联合正态分布往往不可行，因为联合正太分布中还含有两个分量间相互关联的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 协方差与相关系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 协方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二维联合分布中除含有各分量的边际分布外，还含有两个分量间相互关联的信息。描述这种相互关联程度的一个特征数就是协方差，它的定义如下:\n",
    "\n",
    "设$(X,Y)$是一个二维随机变量，若$E[(X-E(X))(Y-E(Y))]$存在，则称此数学期望为$X$与$Y$的协方差，或称为$X$与$Y$的相关(中心)矩，并记为$$Cov(X,Y) = E[(X-E(X))(Y-E(Y))]$$特别有$Cov(X,X) = Var(X)$.\n",
    "\n",
    "从协方差的定义可以看出，它是$X$的偏差\"$X-E(X)$\"与$Y$的偏差\"$Y-E(Y)$\"乘积的数学期望。由于偏差可正可负，故协方差也可正可负，也可为零，其具体表现如下:\n",
    "1. 当$Cov(X,Y)>0$时，称$X$与$Y$正相关，这时两个偏差\"$X-E(X)$\"与\"$Y-E(Y)$\"有同时增加或同时减少的倾向。由于$E(X)$与$E(Y)$都是常数，故等价于$X$与$Y$有同时增加或同时减少的倾向，这就是**正相关**的含义.\n",
    "2. 当$Cov(X,Y)<0$时，称$X$与$Y$负相关，这时有$X$增加而$Y$减少的倾向，或有$Y$增加而$X$减少的倾向，这就是**负相关**的含义.\n",
    "3. 当$Cov(X,Y)=0$时，称$X$与$Y$**不相关**.这时可能由两种情况导致:一类是$X$与$Y$的取值毫无关联，另一类是$X$与$Y$间存有某种非线性关系."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相关系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设$(X,Y)$是一个二维随机变量，且$Var(X)=\\sigma_{X}^{2}>0$,$Var(Y)=\\sigma_{Y}^{2}>0$,则称\n",
    "\n",
    "$$ \\begin{eqnarray}Corr(X,Y)\n",
    "&=& \\frac{Cov(X,Y)}{\\sqrt{Var(X)}\\sqrt{Var(Y)}} \\\\\n",
    "&=& \\frac{Cov(X,Y)}{\\sigma_{X}\\sigma_{Y}}\n",
    "\\end{eqnarray}$$\n",
    "为$X$与$Y$的(线性)**相关系数**。\n",
    "\n",
    "相关系数$Corr(X,Y)$与协方差$Cov(X,Y)$是同符号的，即同为正，或同为负，或同为零。这说明，从相关系数的取值也可反映出$X$与$Y$的正相关、负相关和不相关。\n",
    "\n",
    "相关系数的另一个解释:它是相应标准化变量的协方差。若记$X$与$Y$的数学期望分别为$\\mu_{X}$,$\\mu_{Y}$,其标准化变量为\n",
    "\n",
    "$$X^{*} = \\frac{X-\\mu_{X}}{\\sigma_{X}},Y^{*} = \\frac{Y-\\mu_{Y}}{\\sigma_{Y}},$$\n",
    "则有$$ \\begin{eqnarray}Cov(X^{*},Y^{*})\n",
    "&=& Cov(\\frac{X-\\mu_{X}}{\\sigma_{X}},\\frac{Y-\\mu_{Y}}{\\sigma_{Y}})\\\\\n",
    "&=& \\frac{Cov(X,Y)}{\\sigma_{X}\\sigma_{Y}}=Corr(X,Y).\\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n元正态分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设$n$维随机变量$X = (X_{1},X_{2},...,X_{n})^{T}$的协方差矩阵为$B=Cov(X)$,数学期望向量为$\\alpha = (\\alpha_{1},\\alpha_{2},...,\\alpha_{n})^{T}$.又记$x = (x_{1},x_{2},...,x_{n})^{T}$,则由概率密度函数\n",
    "$$p(x_{1},x_{2},...,x_{n}) = p(x) = \\frac{1}{\\sqrt{(2\\pi)^{n}\\left |B \\right |}}exp\\left \\{-\\frac{1}{2}(x-\\alpha)^{T}B^{-1}(x-\\alpha) \\right \\}$$\n",
    "定义的分布称为**$n$元正态分布**，记为$X\\sim N(\\alpha,B)$.其中$\\left|B\\right|$表示$B$的行列式，$B^{-1}$表示$B$的逆阵，$(x-\\alpha)^{T}$表示向量$(x-\\alpha)$的转置.\n",
    "\n",
    "在$n=2$的场合，若取**数学期望向量**和**协方差矩阵**分别为\n",
    "$$\\alpha = \\begin{pmatrix}\n",
    "\\mu_{1}\\\\ \n",
    "\\mu_{2}\\\\\n",
    "\\end{pmatrix},B = \\begin{pmatrix}\n",
    "\\sigma_{1}^{2} & \\sigma_{1}\\sigma_{2}\\rho \\\\ \n",
    "\\sigma_{1}\\sigma_{2}\\rho & \\sigma_{2}^{2} \\\\\n",
    "\\end{pmatrix},$$代入上述概率密度公式，便可得到二元正态概率密度函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机向量的数学期望向量与协方差矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记**$n$维随机向量**为$X = (X_{1},X_{2},...,X_{n})^{T}$,若其每个**分量**的数学期望都存在，则称$$E(X) = (E(X_{1}),E(X_{2}),...,E(X_{n}))^{T}$$为**$n$维随机向量$X$**的**数学期望向量**，简称为$X$的**数学期望**，而称\n",
    "\n",
    "$$E\\left[(X-E(X))(X-E(X))^{T}\\right] = \\begin{pmatrix}\n",
    "Var(X_{1}) & Cov(X_{1},X_{2}) & \\cdots & Cov(X_{1},X_{n}) \\\\\n",
    "Cov(X_{2},X_{1}) & Var(X_{2}) & \\cdots & Cov(X_{2},X_{n}) \\\\\n",
    "\\vdots & \\vdots &  & \\vdots \\\\\n",
    "Cov(X_{n},X_{1}) & Cov(X_{n},X_{2}) & \\cdots & Var(X_{n}) \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "为该随机向量的**方差-协方差矩阵**，简称**协方差阵**，记为$Cov(X)$.\n",
    "\n",
    "**$n$维随机向量**的**数学期望**是各分量的数学期望组成的向量，而其**方差**就是由各分量的方差与协方差组成的矩阵，其对角线上的元素就是方差，非对角线元素为协方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于**一元正态分布**，**随机变量**的**均值**和**方差**唯一决定了其分布函数和概率密度函数;对于**多元正态分布**，随机向量的**均值向量**和**协方差阵**唯一决定了其分布函数和概率密度函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高斯过程(Gaussian Process，GP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机试验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个试验(或观察)，若它的结果预先无法确定，就可以称之为**随机试验**，简称**试验**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 样本空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有实验的可能结果组成的集合，称为样本空间，记为$\\Omega$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机变量与随机事件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**随机变量**是\"其值随机会而定\"的变量，它的反面就是**确定性变量**，即**取值遵循某种严格规律的变量**。从绝对意义上讲，许多通常视为确定性变量的量，本质上都有随机性，只是由于随机性干扰不大，以至于在所要求的精度之内，不妨把它作为确定性变量来处理。\n",
    "\n",
    "**随机变量**(random variable)表示随机试验各种结果的实值单值函数。随机事件不论与数量是否直接有关，都可以数量化。简单来说，随机变量是指**随机事件**的数量表现。\n",
    "\n",
    "随机变量相当于在样本空间中的一次采样，采样的结果是一个事件，便是随机事件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 幂集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "幂集(power set)是原集合中所有**子集(包括全集和空集)**构成的集族。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指标集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设一集合$I$，若对于每个$i \\in I$,都对应了一个集合$A_{i}$，则由这些$A_{i}$的全体构成的集合$A$称之为**集合族**，$I$就是该集合族的**指标集**。也就是说在 集合$I$中任取一个元素$i$，都可以得到一个集合$A_{i}$ .指标集$I$以实数形式表示时,随机过程是**连续随机过程**；以整数形式表示时，随机过程为**离散随机过程**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指数集(index set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指数集，其成员用来标记另一个集合的成员，就是下标的集合。 \n",
    "\n",
    "指数集和指标集相似:\n",
    "1. 对于指数集，集合$I$中任取一个元素$i$，都可以得到集合$A$中的一个$A_{i}$元素 \n",
    "2. 对于指标集，集合$I$中任取一个元素$i$，都可以得到一个集合$A_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可测空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有实验的可能结果组成的集合，称为样本空间，记为$\\Omega$,设$F$为由$\\Omega$的某些子集构成的非空集类，若满足:\n",
    "1. 若$A \\in F$,则$A^{C} \\in F$,$A^{C}$是$A$的补集,即$A^{C} = \\Omega - A $\n",
    "2. 若$A_{n} \\in F,n \\in N$,则$\\bigcup_{n=1}^{\\infty}A_{n} \\in F $\n",
    "\n",
    "则称$F$为$\\sigma$域，称($\\Omega,F$)为**可测空间**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概率空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概率空间$(\\Omega,F,P)$是一个总**测度**为**1**的测度空间(即$P(\\Omega)=1$).\n",
    "1. $\\Omega$是一个**非空集合**，有时称作**样本空间**，其元素称为**样本输出**，记为$\\omega$\n",
    "2. $F$是样本空间$\\Omega$的幂集的一个非空子集，$F$集合的元素称为**随机事件$\\Sigma$**\n",
    "3. $P$称为概率，或概率测度，它是一个定义在$F$上的集函数,即从集合$F$到实数域$R$的函数: $P: F \\mapsto R$(映射关系) 。每个事件都被此函数赋予一个0和1之间的概率值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，把**一组随机变量**定义为**随机过程**，比如马尔科夫过程。随机过程是许多随机变量的集合，代表了某个随机系统随着某个指示向量的变化，这个指示向量常用的是时间向量。随机过程主要研究**无穷多个互相不独立、有一定相关关系**的**随机变量**。\n",
    "\n",
    "随机过程定义:\n",
    "\n",
    "设$(\\Omega,F,P)$为一**概率空间**，集合$T$为一**指标集合**，若对每一个$t \\in T$,都有一随机变量$X(t,\\omega)$定义在$(\\Omega,F,P)$上$(\\omega \\in \\Omega)$,则称依赖于参数$t \\in T$的**随机变量簇**$\\left \\{X(t,\\omega),t \\in T \\right \\}$为一个定义在概率空间$(\\Omega,F,P)$上的**随机过程**。\n",
    "\n",
    "可以看出$X$是定义在**$T\\times\\Omega$**上的**二元单值函数**。固定$t\\in T$,$X(t,\\cdot)$是定义在样本空间$\\Omega$上的函数，即为一**随机变量**。对于$\\omega \\in \\Omega$,$X(\\cdot,\\omega)$($t$在$T$中顺序变化)是参数$t\\in T$的一般函数，通常称$X(\\cdot,\\omega)$为**样本函数**，或**随机过程的一个实现**，或是一条轨道。\n",
    "\n",
    "$X(t,\\omega)$有时也写作$X_{t}(\\omega)$或简记为$X(t)$或$X_{t}$。$X_{t}(t \\in T)$可能取值的全体所构成的集合称为**状态空间**，记作$S$,其中的每个元素称为**状态**。\n",
    "\n",
    "通常的指标集合是指时间，以实数或整数表示其元素。以实数形式表示时，随机过程即为连续随机过程；以整数形式表示时，随机过程即为离散随机过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机过程和随机变量的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**随机过程**和**随机变量**的区别是:\n",
    "1. 随机变量相当于在样本空间里采样，其采样的对象是一个一个单个事件\n",
    "2. 随机过程在样本空间里采样的是一个**过程**，是一串事件的组合。**随机过程**每次采样的结果是一个过程，比如一个序列，一个时间的函数等。\n",
    "3. 随机变量采样的结果是一个事件，随机过程采样的结果是一串事件\n",
    "\n",
    "我们用图片来进一步说明两者的差异:\n",
    "![Gaussian_process](./image/Gaussian_process.png)\n",
    "\n",
    "图片来源:[A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning](https://arxiv.org/pdf/1012.2599.pdf)\n",
    "\n",
    "图片中:\n",
    "1. 样本空间便是蓝紫色区域\n",
    "2. 在蓝紫色区域随便画一条函数曲线，便是一个可能的随机过程\n",
    "3. 蓝紫色区域的上下界是当前样本点均值的一个标准差范围，即$\\left (\\mu(X_{n}) - \\sigma(X_{n}),\\mu(X_{n}) + \\sigma(X_{n})) \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图便是10个随机过程(具体而言是高斯过程):\n",
    "![Gaussian_process2](./image/Gaussian_process2.png)\n",
    "\n",
    "曲线交叉的地方，或收窄的地方，是实际的观测值，方差较小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高斯过程：在函数上的正态分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高斯过程指的是一组随机变量的集合，这个集合里面的任意有限个随机变量都服从联合正态分布。具体地，对概率空间$(\\Omega,F,P)$内指数集为$T$的随机过程$X = \\left \\{ X_{t}\\right \\}_{t \\in T}$,当$X$的子集$(X_{t_{1}},...,X_{t_{n}})$对任意$\\left \\{t_{1},...,t_{n} \\right \\} \\in T,n \\geq 1$ 都是高斯随机向量时，$\\left \\{X_{t} \\right \\}$被称为**高斯过程**。\n",
    "\n",
    "有如下引理:\n",
    "对**高斯随机向量**$X = (X_{1},...,X_{n})$,若有**指数集**$T = \\left \\{1,...,n \\right \\}$,则随机过程$X = \\left \\{ X_{t}\\right \\}_{t \\in T}$ 是**高斯过程**；反之，若随机过程 $\\left \\{ X_{t}\\right \\}_{t \\in T}$是**高斯过程**，则$X = (X_{1},...,X_{n})$是**高斯随机向量**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机过程可以用一个**随机变量簇$X = \\left \\{ X_{t}\\right \\}_{t \\in T}$**来表示，而**高斯过程区别于其它随机过程的地方在于**:从这个随机变量簇中任意抽取有限个指标(属于指标集，如$t_{1},...,t_{n}$)所得到的变量构成的向量$X = (X_{t_{1}},...,X_{t_{n}})$的联合分布为**多维高斯分布**。由正态分布的可加性知，高斯过程中**任意随机变量的线性组合**也是高斯过程;**每个有限维分布**都服从**联合正态分布**,因此**高斯过程**可以看做**多维高斯分布向无限维的扩展**。\n",
    "\n",
    "**怎么理解\"高斯过程的每个有限维分布都服从联合正态分布\"呢？**\n",
    "\n",
    "我们先来看一个随机序列:$\\left \\{X_{1},X_{2},...,X_{n} \\right \\}$是一个维度为$n$的序列，它可以理解为是一个无穷序列$\\left \\{ X_{t}\\right \\}_{t \\in T}$进行的$n$次采样。 $ t \\in T$在这里可以理解为时间，更准确的应该理解为**一个连续的指标集**。$\\left \\{X_{1},X_{2},...,X_{n} \\right \\}$ 因其一般性，就可以看成$\\left \\{ X_{t}\\right \\}_{t \\in T}$的**有限维分布**。如果$\\left \\{ X_{t}\\right \\}_{t \\in T}$是一个高斯过程，则$\\left \\{X_{1},X_{2},...,X_{n} \\right \\}$必然服从$n$元正态分布。\n",
    "\n",
    "高斯过程由**随机向量的数学期望**和**协方差函数(也叫核函数)**完全决定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在概率论和统计学中，高斯过程是一个随机过程，它是由**时间或空间索引的随机变量的集合**，使得这些随机变量的**每个有限集合具有多元正态分布**，即它们的每个**有限线性组合**通常是**正态分布**。 **高斯过程的分布**是所有那些（**无限多个**）随机变量的联合分布，因此，它是具有**连续域的函数的分布**，例如，时间或空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高斯过程是一种常用的监督学习方法，旨在解决**回归问题**和**概率分类问题**，即高斯过程回归和高斯过程分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高斯过程回归(GPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习过程中，我们通常假设我们的问题满足一个高斯过程，并且可以通过核函数来“拓展”高斯过程对问题的表示能力。也就是说，**高斯过程回归**将回归模型所对应的函数空间视为高斯过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 核函数的通俗解释：SVM和高斯过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**和**高斯过程**中的**核函数(kernel function)**是通用的，指代的是同一对象，在**sklearn.gaussian_process.kernels**可以找到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM中的核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在**SVM**中**核函数**的作用是计算将两个低维向量经过一定映射变换后的高维向量，在高维向量空间的内积。核函数是二元函数，输入是原始的两个向量，输出则是两个向量映射到高维向量的内积。核函数有如下形式:\n",
    "$$K(x_{1},x_{2}) = \\phi(x_{1})*\\phi(x_{2})$$\n",
    "其中,$\\phi(x)$是$x$的映射函数，等式右边是映射到高维向量空间的两个向量的内积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>当训练数据**线性可分**时，我们来推一下**线性SVM**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不考虑软边距的松弛变量，我们简单来推一下线性的硬边距SVM，就能更清晰的知道核函数的作用了:\n",
    "\n",
    "在分类问题中，给定输入数据$X = \\left \\{X_{1},...,X_{N} \\right \\}$和学习目标$y = \\left \\{y_{1},...,y_{N} \\right \\}$,其中输入数据的每个样本都包含多个特征并由此构成特征空间，即$X_{i} = \\left [x_{1},...,x_{n} \\right ] \\in \\chi$,而学习目标为二元变量$y \\in \\left \\{-1,1 \\right \\}$表示负类和正类。若输入数据所在的特征空间存在作为**决策边界**的**超平面**:$$\\omega^{T}X + b = 0$$\n",
    "它使任意样本的点到超平面的距离大于等于1，即$$y_{i}\\left(\\omega^{T}X + b \\right) \\geq 1$$\n",
    "则称该分类问题具有线性可分性，参数$\\omega,b$分别为超平面的法向量和截距，如下图所示:\n",
    "![svm](./image/svm.jpg)\n",
    "满足该条件的决策边界实际上构造了两个平行的超平面:$$\\omega^{T}X + b = \\pm 1$$作为**间隔边界**以判别样本的类别:$$\\omega^{T}X_{i} + b -1 \\geq +1,if,y_{i} = +1$$ \n",
    "$$\\omega^{T}X_{i} + b -1 \\leq -1,if,y_{i} = -1 $$\n",
    "\n",
    "所有在间隔边界上方的样本属于正类，在间隔下方的样本属于负类。**两个间隔边界的距离**$$d = \\frac{2}{||\\omega||}$$被定义为**边距**，在这两个间隔边界上的正负样本点称为**支持向量**。\n",
    "\n",
    "学习目标便理所当然的是让边距$$d = \\frac{2}{||\\omega||}$$最大化，即优化目标以及其满足的条件如下:\n",
    "$$\n",
    "\\left\\{\\begin{matrix}\n",
    "\\underset{\\omega,b}{max}&\\frac{2}{||\\omega||} &  &\\underset{\\omega,b}{min}&\\frac{1}{2}||\\omega||^{2} \\\\ \n",
    "&&\\Leftrightarrow && \\\\\n",
    "s.t.&y_{i}(\\omega^{T}X_{i} + b) \\geq 1 &   &s.t.&y_{i}(\\omega^{T}X_{i} + b) \\geq 1\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "由上式可得到的决策函数可以对任意样本进行分类，只需代入$ sign\\left [y_{i}(\\omega^{T}X_{i} + b) \\right]$,由于$y_{i}$未知，可通过计算括号内的值，再依据$y_{i}(\\omega^{T}X_{i} + b) \\geq 1$来判断$y_{i}$的值。\n",
    "\n",
    "定义硬边距SVM的优化问题为原问题,通过**拉格朗日乘子**:$\\alpha = \\left \\{\\alpha_{1},...,\\alpha_{N} \\right \\}$可得到其**拉格朗日函数**:\n",
    "$$L \\left(\\omega,b,\\alpha \\right) = \\frac{1}{2}||\\omega||^{2} + \\sum_{i=1}^{N}\\alpha_{i}\\left [1 - y_{i}(\\omega^{T}X_{i} + b) \\right ]$$\n",
    "令拉格朗日函数对优化目标$\\omega,b,\\alpha$的偏导数为0，可得到一系列包含拉格朗日乘子的表达式:\n",
    "$$\\frac{\\partial L}{\\omega} = 0 \\Rightarrow \\omega = \\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i} $$\n",
    "$$\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0$$\n",
    "将结果代入原问题的优化目标后可得:\n",
    "$$min : \\frac{1}{2}\\left (\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i} \\right )^{2}$$\n",
    "$$s.t. :\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0 $$\n",
    "其对偶问题是:\n",
    "$$max : - \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\left [\\alpha_{i}y_{i}(X_{i})^{T}(X_{j})y_{i}\\alpha_{j} \\right ]$$\n",
    "$$s.t. :\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0$$\n",
    "\n",
    "再对 对偶问题存在局部最优解的拉格朗日乘子满足**KKT**条件求解即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>当训练数据**线性不可分**时，考虑**非线性的SVM**，此时便是**核函数**大显身手的时候了</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当训练数据线性不可分，即依赖于原有特征是找不到一个**超平面**作决策边界的，此时可以使用**非线性函数$\\phi(x)$**将输入数据$X = \\left \\{X_{1},...,X_{N} \\right \\}$映射至高维空间后，便可以找到一个超平面:$$\\omega^{T}\\phi(X) + b = 0$$\n",
    "作为决策边界，此时的SVM便是非线性SVM,注意每个$X_{i} = \\left [x_{1},...,x_{n} \\right ] \\in \\chi$都是多维特征。\n",
    "\n",
    "**非线性SVM**优化的目标函数及满足的条件为:\n",
    "$$\\underset{\\omega,b}{min}:\\frac{1}{2}||\\omega||^{2}$$\n",
    "$$ s.t. : y_{i}\\left [\\omega^{T}\\phi(X_{i} + b) \\right ] \\geq 1$$\n",
    "其对偶问题变为(当然经过复杂的计算后的结果):\n",
    "$$max : - \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\left [\\alpha_{i}y_{i}\\phi(X_{i})^{T}\\phi(X_{j})y_{i}\\alpha_{j} \\right ]$$\n",
    "$$s.t. :\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0$$\n",
    "注意到式中存在映射函数$\\phi(x)$的**内积**:$$\\phi(X_{i})^{T}\\phi(X_{j})$$\n",
    "因此，**核函数的作用**就是计算映射后向量的内积,即$$ K(x_{i},x_{j}) = \\phi(x_{i})*\\phi(x_{j})$$\n",
    "$\\phi(x_{i})$与$\\phi(x_{j})$就是映射后的高维向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>核函数度量的，表面上是两个向量的内积，实质是两个向量的相似度</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了核函数，我们便可以绕过求出映射函数$\\phi(x)$的具体表达式，而直接求出原向量映射后的高维向量的内积了。根本不需要映射函数$\\phi(x)$的具体表达式，因为我们最终的目的是求**内积**。根据**Mercer**定理，我们可以得到很多核函数，这些核函数对应的映射函数，并不知道，我们根本不需要知道，因为**在整个SVM过程中，我们根本没有将原向量做映射变换，我们需要的只是那个内积而已**。\n",
    "\n",
    "<font color=green>下面我们举个能够找到具体映射函数$\\phi(x)$的**核函数**:</font>\n",
    "$v_{1} = (x_{1},y_{1})$ 和 $v_{2} = (x_{2},y_{2})$为原向量,我们的**核函数**为:\n",
    "$$K(v_{1},v_{2}) = (v_{1}\\cdot v_{2})^{2}$$\n",
    "即核函数我们定义为原向量内积的平方，它对应的映射函数$\\phi(v)$为:\n",
    "$$\\phi(v) = (x^{2},\\sqrt{2}xy,y^{2})$$\n",
    "可以验证，映射后的向量的内积为:\n",
    "$$\\begin{eqnarray} \\phi(v_{1}) \\cdot \\phi(v_{2})\n",
    "&=& (x_{1}^{2},\\sqrt{2}x_{1}y_{1},y_{1}^{2}) \\cdot (x_{2}^{2},\\sqrt{2}x_{2}y_{2},y_{2}^{2}) \\\\\n",
    "&=& x_{1}^{2}x_{2}^{2} + 2x_{1}x_{2}y_{1}y_{2} + y_{1}^{2}y_{2}^{2} \\\\\n",
    "&=& (x_{1}x_{2} + y_{1}y_{2})^{2} \\\\\n",
    "&=& (v_{1}  \\cdot v_{2})^{2} \\\\\n",
    "&=& K(v_{1},v_{2})\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "下面是一个将原向量映射到高维空间后，线性可分的例子:\n",
    "\n",
    "原向量：原空间中无法找到一个超平面将红色的门和北京四合院下面的紫色字母分开\n",
    "![svm_unlinear1](./image/svm_unlinear1.jpg)\n",
    "\n",
    "映射后高维空间的向量：高维空间里找到了一个超平面来将它们分开\n",
    "![svm_unlinear2](./image/svm_unlinear2.jpg)\n",
    "\n",
    "将该超平面再映射回低维空间后，我们发现它是一个双曲线:\n",
    "![svm_unlinear3](./image/svm_unlinear3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高斯过程中的核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核函数度量的是两个向量的内积，根据两个向量**余弦相似度**的计算公式可知，**内积度量的是两个向量的相似度**，因为我们可以把分母(各个向量的模)放进向量中，即将向量单位化。我们有理由相信，在低维空间相似的向量，映射到高维空间同样相似；或者说，在低维空间靠的近的向量，在高维空间也较近。\n",
    "\n",
    "高斯过程中的**协方差函数**，是随机变量的格拉姆矩阵，即随机变量之间协方差的矩阵，而**协方差度量的是两个变量的相关性**.**我们认为两个相似的随机变量$x$，其对应的$y$值相关性高**。这里的随机变量可以是向量，因此协方差矩阵是随机变量的函数而与随机变量对应的$y$值无关，它是随机变量的函数。\n",
    "\n",
    "由于协方差矩阵度量的是两个随机变量(向量)的相关性，且高斯过程是在函数上的分布，因此$y$越相关，随机变量$x$越相似。而度量相似性的便是向量的内积，核函数应运而出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 概念:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**高斯过程回归**不像常规的回归方法要声明$f(x)$的具体形式，如线性的$f(x) = mx + c$,二次的$f(x) = ax^{2} + bx + c$等具体形式。<font color=red>在高斯过程回归中，不用指定$f(x)$的具体形式，</font>$n$个训练数据的观测值$\\left(y_{1},...,y_{n} \\right)$被认为是从某个多维($n$维)的高斯分布中采样出来的一个点($n$维的)，而类似的$f(x)$也可以认为是从高斯过程中采样得到的一个无穷维的点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 给定训练数据，对观测值$y$建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在给定训练数据$x_{1},...,x_{n}$与其对应的函数值$y_{1},...,y_{n}$,由于观测通常是带噪声的，这里将每个观测$y$建模为某个隐函数$f(x)$加上一个高斯噪声，即$$y = f(x) + N(0,\\sigma_{n}^{2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建高斯过程先验 : 给定 均值函数 和 核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中函数$f(x)$被假设给予一个高斯过程先验，即$$f(x) \\sim GP(\\mu,k)$$\n",
    "其中**$\\mu$**为**均值函数**,**$K$**为**协方差函数(核函数)**\n",
    "\n",
    "这里均值函数假设为0，核函数选用**RBF核**，即\n",
    "$$f(x) \\sim GP(0,k),K(X_{1},X_{2}) = \\sigma_{f(x)}^{2}exp(-\\frac{||X_{1} - X_{2}||^{2}}{2l^{2}})$$\n",
    "\n",
    "**$l$** 为核函数的超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对函数建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据以上可以写出加入噪声后的$K(X_{1},X_{2})$为:\n",
    "$$K(X_{1},X_{2}) = \\sigma_{f(x)}^{2}exp(-\\frac{||X_{1} - X_{2}||^{2}}{2l^{2}}) + \\sigma_{n}^{2}\\delta(X_{1},X_{2})$$\n",
    "至此，完成了基本的模型建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 求解模型中的超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面我们可知，模型中待求的超参数有:$\\theta = \\left \\{l,\\sigma_{f},\\sigma_{n} \\right \\}$。\n",
    "\n",
    "如果在使用高斯过程中假设均值为0，那么结果的好坏与核函数的选择有很大关系。如上面选择的$RBF核$，在模型中总共涉及到的参数$\\theta = \\left \\{l,\\sigma_{f},\\sigma_{n} \\right \\}$。当这些参数选择的不好时，得到的结果可能会毫无意义。\n",
    "\n",
    "求解参数$\\theta$的方法是对参数进行最大后验估计，选择使$p(\\theta|x,y)$最大的$\\theta$，其中:\n",
    "$$ \\begin{eqnarray}p(\\theta|x,y)\n",
    "                   &=& \\frac{p(\\theta,x,y)}{p(x,y)} \\\\\n",
    "                   &=& \\frac{p(y|x,\\theta)p(x,\\theta)}{p(x)p(y|x)} \\\\                   \n",
    "                   &=& \\frac{p(y|x,\\theta)p(x)p(\\theta)}{p(x)p(y|x)} \\\\                   \n",
    "                   &=& \\frac{p(y|x,\\theta)p(\\theta)}{p(y|x)}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "但是假如我们对$\\theta$没有任何先验知识，即$p(\\theta)$不知，那么此时的最大后验估计就行不通了，它退化为最大似然估计,似然为$p(y|x,\\theta)$，也就是我们希望找到能使$p(y|x,\\theta)$最大的$\\theta$。这很简单，因为$x,y$都是已知的，再根据高斯过程$y|x,\\theta \\sim N(0,K)$,则具体的似然式子为:  \n",
    "\n",
    "$$ \\begin{eqnarray} logp(y_{i}|x_{i},\\theta) \n",
    "&=& log\\left [\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left \\{-\\frac{(y_{i}-0)^2}{2\\sigma^{2}} \\right \\} \\right ] \\\\\n",
    "&=& log\\left [exp\\left \\{-\\frac{y_{i}^{2}}{2\\sigma^{2}} \\right \\} \\right ] - log\\left [\\sqrt{2\\pi}\\sigma \\right ] \\\\\n",
    "&=& -\\frac{y_{i}^{2}}{2\\sigma^{2}} - log\\sigma - \\frac{log2\\pi}{2} \\\\\n",
    "&=& -\\frac{y_{i}y_{i}}{2K} - log\\sqrt{|K|} - \\frac{log2\\pi}{2}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "\n",
    "$$ \\begin{eqnarray} logp(y|x,\\theta) \n",
    "&=& -\\frac{y^{T}y}{2K} - log\\sqrt{|K|} - n*\\frac{log2\\pi}{2} \\\\\n",
    "&=& -\\frac{y^{T}K^{-1}y}{2KK^{-1}} - log\\sqrt{|K|} - \\frac{nlog2\\pi}{2} \\\\\n",
    "&=& -\\frac{y^{T}K^{-1}y}{2} - \\frac{log|K|}{2} - \\frac{nlog2\\pi}{2}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "\n",
    "通过共轭梯度等方法来最大化上述似然，可以得到对参数$\\theta$的一个较好的估计。下面的例子就是用这种方法，得到$l=1,\\sigma_{f}=1.27,\\sigma_{n}=0.3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对新数据进行预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的任务是要对新的**$x^{*}$**,求出它所对应的**$y^{*}$**，注意**不是$ f(x^{*})$，或$f^{*}$**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y\\\\ \n",
    "y^{*}\n",
    "\\end{bmatrix} \\sim N\\left(0,\\begin{bmatrix}\n",
    "K & K_{*}^{T}\\\\ \n",
    "K_{*} & K_{**}\n",
    "\\end{bmatrix} \\right)\n",
    "$$\n",
    "其中\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "k(x_{1},x_{1}) & k(x_{1},x_{2})  & \\cdots  & k(x_{1},x_{n}) \\\\ \n",
    "k(x_{2},x_{1}) & k(x_{2},x_{2})  & \\cdots  & k(x_{2},x_{n}) \\\\ \n",
    "\\cdots & \\cdots & \\cdots & \\cdots\\\\ \n",
    "k(x_{n},x_{1}) & k(x_{n},x_{2})  & \\cdots & k(x_{n},x_{n})\n",
    "\\end{bmatrix}  \n",
    "$$\n",
    "\n",
    "$$K_{*} = \\left [k(x_{*},x_{1}),k(x_{*},x_{2}) ,... ,k(x_{*},x_{n}) \\right ],K_{**} = k(x_{*},x_{*})$$\n",
    "\n",
    "有了联合分布，接下来就可以比较容易的求出预测数据$y^{*}$的条件分布$p(y^{*}|y)$,经过推导可以得到其条件分布也是高斯分布，如下\n",
    "\n",
    "$$y^{*}|y \\sim N\\left(K_{*}K^{-1}y,K_{**} - K_{*}K^{-1}K_{*}^{T} \\right)$$\n",
    "\n",
    "对$y^{*}$的估计，我们就用分布的均值来作为其估计值，即$$\\bar{y_{*}} = K_{*}K^{-1}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 核函数(kernel function)的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核函数的选择必须满足**Mercer定理**:核函数在样本空间内的任意**格拉姆矩阵(Gram matrix)**为半正定矩阵。\n",
    "\n",
    "Gram矩阵的定义:\n",
    "$n$维欧式空间中任意$k(k \\leq n)$个向量$\\alpha_{1},\\alpha_{2},...,\\alpha_{k}$的内积所组成的矩阵\n",
    "$$\n",
    "\\Delta(\\alpha_{1},\\alpha_{2},...,\\alpha_{k}) = \\begin{pmatrix}\n",
    "(\\alpha_{1},\\alpha_{1}) & (\\alpha_{1},\\alpha_{2})  & \\cdots  & (\\alpha_{1},\\alpha_{k}) \\\\ \n",
    "(\\alpha_{2},\\alpha_{1}) & (\\alpha_{2},\\alpha_{2})  & \\cdots  & (\\alpha_{2},\\alpha_{k}) \\\\ \n",
    "\\cdots & \\cdots & \\cdots & \\cdots\\\\ \n",
    "(\\alpha_{k},\\alpha_{1}) & (\\alpha_{k},\\alpha_{2})  & \\cdots & (\\alpha_{k},\\alpha_{k})\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "称为$k$个向量$\\alpha_{1},\\alpha_{2},...,\\alpha_{k}$的格拉姆矩阵，它的行列式为格拉姆行列式。\n",
    "\n",
    "格拉姆矩阵可以看做特征之间的协方差矩阵,而内积往往是两个点之间相似度的度量，因此格拉姆矩阵用于度量各个维度之间的关系。\n",
    "\n",
    "正定矩阵和半正定矩阵:\n",
    "1. 矩阵$A$为正定矩阵是指，对任意的$X \\neq 0$恒有$X^{T}AX > 0$\n",
    "2. 矩阵$A$为半正定矩阵是指，对任意的$X \\neq 0$恒有$X^{T}AX \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RBF核函数**(高斯核函数，径向基核函数，平方指数核函数)\n",
    "$$K(X_{1},X_{2}) = exp(-\\frac{||X_{1} - X_{2}||^{2}}{2\\sigma^{2}})$$\n",
    "$\\sigma$为$RBF$核的超参数，定义了学习样本间相似性的特征长度尺度，即权重空间视角下特征空间映射后前后样本间距离的比例。\n",
    "\n",
    "**常数核**(constant kernel):\n",
    "$$K(X_{1},X_{2}) = C$$\n",
    "$C$是一个常数\n",
    "\n",
    "**线性核函数**(linear kernel):\n",
    "$$K(X_{1},X_{2}) = X_{1}^{T}X_{2}$$\n",
    "\n",
    "**多项式核函数**:\n",
    "$$K(X_{1},X_{2}) = (aX_{1}^{T}X_{2} + c)^{b}$$\n",
    "$b=1,2,..N$,  其中$a,c$都是参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核函数不仅可以采用上面常用的单一形式，还可以采用上面常用核函数的**组合形式**。如下图3(a)\n",
    "![kernel_function](./image/kernel_function.png)\n",
    "就可以采用如下形式的核函数$$K(X_{1},X_{2}) = \\sigma_{f_{1}}^{2}exp\\left [\\frac{-(X_{1} - X_{2})^{2}}{2l_{1}^{2}} \\right ] + \\sigma_{f_{2}}^{2}exp\\left [\\frac{-(X_{1} - X_{2})^{2}}{2l_{2}^{2}} \\right ] + \\sigma_{n}^{2}\\delta(X_{1},X_{2})$$\n",
    "上式中的第一项可以用来表示隐变量的小的波动(每个小的波峰波谷)，第二项可以用来表示长期的变化(整体性的向下趋势)。\n",
    "\n",
    "对图3b可以观察到其中数据具有明显的周期性特点，因此可以采用如下形式的核函数:\n",
    "$$K(X_{1},X_{2}) = \\sigma_{f}^{2}exp\\left [\\frac{-(X_{1} - X_{2})^{2}}{2l^{2}} \\right ] + exp\\left \\{-2sin^{2}\\left [\\upsilon \\pi(X_{1} - X_{2}) \\right ] \\right \\} + \\sigma_{n}^{2}\\delta(X_{1},X_{2})$$\n",
    "\n",
    "上式的第一项用来表示整体的像山一样的长期整体趋势，第二项用来表示其中的周期性部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在已经观察到了6个样本点，$x$为样本点的特征(这里特征是一维的)，$y$为样本输出值。现在新来了一个样本点$x^{*}$，要求用**高斯过程回归**来预测新样本点的输出值$y^{*}$。这些样本点显示如下:\n",
    "![case_gaussian](./image/case_gaussian.png)\n",
    "\n",
    "其中，前面6个点是已知输出值的训练样本，其值为:$x = \\left[-1.50,-1.00,-0.75,-0.40,-0.25,0.00 \\right]$,同时这些$x$对应的$y_{n}$值也知道，$\\sigma_{n} = 0.3$.图中红色的竖线表示观测输出值的误差，绿色的竖线为用高斯过程回归的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用GPR解决该问题的大致流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 选择适当的**均值函数$\\mu$**,**核函数K**,**噪声变量**的方差$\\sigma_{n}^{2}$,$\\sigma_{f}$,核函数的**超参数$l$**;核函数的选择尤其重要，因为它体现了需处理问题的先验知识，应根据不同的应用选用不同的核\n",
    "    \n",
    "2. 计算出6个训练样本的**核矩阵**，如下:\n",
    "![kernel_matrix](./image/kernel_matrix.png)\n",
    "\n",
    "3. 计算需预测的点$x^{*}=0.2$ 与6个训练样本$x_{n}$的核值向量，如下:\n",
    "$$K_{*} = \\left[0.38,0.79,1.03,1.35,1.46,1.58 \\right] ， K_{**} = 1.70$$\n",
    "\n",
    "4. 此时6个训练数据的$y_{n}$与待求的$y^{*}$的联合高斯分布表达式为\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y\\\\ \n",
    "y^{*}\n",
    "\\end{bmatrix} \\sim N\\left(0,\\begin{bmatrix}\n",
    "K & K_{*}^{T}\\\\ \n",
    "K_{*} & K_{**}\n",
    "\\end{bmatrix} \\right)\n",
    "$$\n",
    "\n",
    "5. 有了联合分布，接下来就可以比较容易的求出预测数据$y^{*}$的条件分布$p(y^{*}|y)$,经过推导可以得到其条件分布也是高斯分布，如下\n",
    "$$y^{*}|y \\sim N\\left(K_{*}K^{-1}y,K_{**} - K_{*}K^{-1}K_{*}^{T} \\right)$$对$y^{*}$的估计，我们就用分布的均值来作为其估计值，即$$\\bar{y_{*}} = K_{*}K^{-1}y$$根据均值和方差的表达式，我们可以求得均值为0.95，方差为0.21\n",
    "\n",
    "6. 最终结果如下:\n",
    "![case_result](./image/case_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高斯过程回归:sklearn.gaussian_process.GaussianProcessRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GaussianProcessRegressor（kernel,alpha,optimizer,n_restarts_optimizer,normalize_y）**类实现了回归情况下的高斯过程(GP)模型。为此，需要**事先指定GP的先验**:即均值和核函数。\n",
    "\n",
    "模型参数解释如下:\n",
    "1. 参数kernel,是**先验的方差**，核函数\n",
    "2. 参数alpha,是噪声级别\n",
    "3. 参数optimizer,是估计模型超参数的优化器\n",
    "4. 参数n_restarts_optimizer，是指定超参数优化重复的次数，因为超参数可能存在多个局部最优解\n",
    "5. 参数normalize_y=False，先验的均值通常假定为**常数或者零**；参数normalize_y=True，先验均值通常为**训练数据的均值**\n",
    "\n",
    "通过最大化**基于传递optimizer参数**的**对数边缘似然估计(LML)**，内核的超参可以在 GaussianProcessRegressor类执行拟合过程中被优化。\n",
    "\n",
    "由于 LML 可能会存在多个局部最优解，因此优化过程可以通过指定**n_restarts_optimizer参数**进行多次重复。\n",
    "\n",
    "通过设置内核的超参初始值来进行第一次优化的运行。后续的运行过程中超参值都是从合理范围值中随机选取的。如果需要保持初始化超参值， 那么需要把优化器设置为None 。\n",
    "\n",
    "目标变量中的**噪声级别**通过**参数alpha**来传递并指定，要么全局是常数要么是一个数据点。 请注意，适度的噪声水平也可以有助于处理拟合期间的数字问题，因为它被有效地实现为吉洪诺夫正则化(Tikhonov regularization)， 即通过将其添加到核心矩阵的对角线。明确指定噪声水平的替代方法是将**WhiteKernel组件包含在内核中**， 这可以从数据中估计全局噪声水平。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造回归的伪回归数据:sklearn.datasets.make_friedman2包含了特征的乘积和互换操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,Y = make_friedman2(n_samples=100,noise=0,random_state=0)\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造多个核函数的组合形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import DotProduct,WhiteKernel\n",
    "\n",
    "kernel = DotProduct() + WhiteKernel() # 内积核+白核"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用高斯过程回归GPR来训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
       "             kernel=DotProduct(sigma_0=1) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=0, normalize_y=False,\n",
       "             optimizer='fmin_l_bfgs_b', random_state=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "model = GaussianProcessRegressor(kernel=kernel,random_state=0)\n",
    "model.fit(X_train,Y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用高斯过程回归来预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([377.24968381, 258.74131889, 165.02456987,  91.53037955,\n",
       "        133.93038071, 666.83571405, 700.21301898, 723.76371274,\n",
       "        734.83581649, 303.23192804,  90.73487222, 642.95011804,\n",
       "        735.79295355, 138.47173758, 548.60223155, 728.70160876,\n",
       "        565.60535824, 464.20746521, 499.16740312, 412.71433092,\n",
       "        150.25487268, 172.30666976, 499.24485932, 171.13397652,\n",
       "        386.47174932, 362.69920491, 495.09088531, 524.40689562,\n",
       "        284.89039919, 242.65495803]),\n",
       " array([304.52565338, 305.27482803, 310.34491292, 305.79399172,\n",
       "        303.58099401, 311.36111742, 311.07903458, 310.02723994,\n",
       "        312.02452437, 305.73579779, 307.43866226, 311.77663857,\n",
       "        311.57134714, 303.37273492, 309.00777664, 308.04955674,\n",
       "        308.56390474, 305.2469205 , 306.67876772, 306.73318384,\n",
       "        303.55386338, 303.76300106, 305.61057016, 308.32362937,\n",
       "        304.84455914, 307.82609901, 306.94697596, 307.09481182,\n",
       "        305.24565407, 303.75845271]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test,return_std=True)# return_std=True返回查询点处预测分布的标准偏差以及均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个返回值是均值，shape = (n_samples, n_output_dims)，第二个返回值是标准偏差，shape = (n_samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯优化理论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们已经有了几个油井，每个油井的价值不一样，我们在这个二维平面上，利用高斯过程回归，对每一个地理位置估计一个该位置对应的出油量。而开发每一口井是有成本的，在预算有限的情况下，如果想尽可能少地花钱，我们就需要定义一个**效益函数**，同高斯过程回归的预测结果相结合，来指导我们下一次在哪儿打井。这个效益函数往往是**预测值**和**方差**的一个**函数**。以上这个例子，就是**高斯过程回归**在**贝叶斯优化**中的一个典型应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯优化思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯优化是一种**逼近思想**，当计算非常复杂、迭代次数较高时能起到很好的效果，多用于超参数确定。\n",
    "\n",
    "**贝叶斯优化用于机器学习调参** 由J. Snoek(2012)提出，**主要思想**是:给定优化的目标函数(广义的函数，只需指定输入和输出，无需知道内部结构以及数学性质)，**通过不断地添加样本点来更新目标函数的后验分布**(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。\n",
    "\n",
    "换种说法就是**基于数据使用贝叶斯定理估计目标函数的后验分布**，然后**再根据分布选择下一个采样的超参数组合**。它充分利用了前一个采样点的信息，其优化的工作方式是通过对目标函数形状的学习，并找到使结果向全局最大提升的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯优化调参 与 随机搜索、网格搜索的比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 贝叶斯调参采用高斯过程，考虑之前的参数信息，不断地更新后验；网格搜索未考虑之前的参数信息\n",
    "2. 贝叶斯调参迭代次数少，速度快；网格搜索速度慢,参数多时易导致维度爆炸\n",
    "3. 贝叶斯调参针对非凸问题依然稳健；网格搜索针对非凸问题易得到局部优最"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯优化调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高斯过程回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用于对目标函数建模，得到其后验分布。\n",
    "\n",
    "给定参数组合，以及各个参数的范围，这里的各个参数便是随机变量，而用于拟合优化的目标函数便是$y$值，利用高斯过程回归，得到目标函数的后验分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贝叶斯优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过高斯过程建模之后，我们尝试抽样进行样本计算，而我们很容易利用当前的信息，在当前局部最优解上不断采样，这就涉及到了开发和探索之间的权衡。**贝叶斯优化包含了开发和探索，花最少的代价找到最优值**。 利用当前局部最优解的信息，来开发和探索更佳的全局最优解，便体现了贝叶斯思想。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开发(exploitation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据后验分布，在**最可能出现全局最优解的区域进行采样**, 开发高意味着均值高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 探索(exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在**还未取样的区域**获取采样点， 探索高意味着方差高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquistion Function（AC函数）：采样函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何高效的采样，即开发和探索，我们需要用到**Acquisition Function**, 它是用来寻找下一个**x**的函数,这里的$x$是我们待求的参数组合。\n",
    "\n",
    "一般形式的Acquisition Funtion是关于**$x$**的函数，映射到实数空间R，表示**新样本点的目标函数值能够比当前最优值大多少的概率**，目前主要有以下几种主流的Acquisition Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POI(probability of improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$POI(X) = P(f(X) \\geq f(X^{+}) + \\xi) = \\Phi(\\frac{\\mu(x) - f(X^{+}) - \\xi}{\\sigma(x)})$$\n",
    "\n",
    "其中，$f(X)$为$X$的**目标函数值**，$f(X^{+})$为**到目前为止最优的**$X$的**目标函数值**；$\\mu(x),\\sigma(x)$分别为高斯过程所得到的目标函数的**均值和标准差**，即$f(X)$的**后验分布**。\n",
    "\n",
    "$\\xi$为**trade-off**系数，如果没有该系数，$POI$函数会倾向于取在$X^{+}$周围的点，倾向于**开发**而不是**探索**，故加入该项进行权衡。\n",
    "\n",
    "我们要做的，就是尝试**新的$X$**,使得$POI(X)$最大，则采取该$X$，因为$f(X)$的计算代价非常大，通常我们使用**蒙特卡洛模拟**的方法进行，详细情况见下图:\n",
    "![Gaussian_process](./image/Gaussian_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EI(Expected Improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$POI$是一个概率函数，因此只考虑了$f(X)$比$f(X^{+})$大的概率，而$EI$则是一个期望函数，因此考虑了$f(X)$比$f(X^{+})$大多少。我们通过下式获取$X$:\n",
    "\n",
    "$$x = argmax_{x}E\\left(max \\left \\{0,f_{t+1}(x) - f(X^{+}) \\right \\}|D_{t} \\right)$$\n",
    "\n",
    "其中，$D_{t}$为前$t$个样本，在正态分布的假定下，最终得到:\n",
    "\n",
    "$$EI(x) = \\left\\{\\begin{matrix}\n",
    "(\\mu(x) - f(X^{+}))\\Phi(Z) + \\sigma(x)\\phi(Z) , if ,\\sigma(x) > 0\\\\ \n",
    "0 , if , \\sigma(x) = 0\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "其中,$Z = \\frac{\\mu(x) - f(x^{+})}{\\sigma(x)}$,$\\phi(Z)$为$Z$的概率密度函数，$\\Phi(Z)$为$Z$的分布函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence bound criteria：置信区间原则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下置信区间:\n",
    "$$LCB(x) = \\mu(x) - k\\sigma(x)$$\n",
    "上置信区间:\n",
    "$$UCB(x) = \\mu(x) + k\\sigma(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bayesian-optimization包的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install bayesian-optimization ， **github地址**:https://github.com/fmfn/BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造伪分类数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 构造数据集\n",
    "X,Y = make_classification(n_samples=1000,\n",
    "                          n_features=5,\n",
    "                          n_informative=3,\n",
    "                          n_redundant=1,\n",
    "                          n_repeated=1,\n",
    "                          n_classes=2,\n",
    "                         )\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不使用贝叶斯优化的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiyang/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/weiyang/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/weiyang/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/weiyang/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/weiyang/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/weiyang/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/weiyang/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/weiyang/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/weiyang/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/weiyang/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.95515873, 0.94081633, 0.98122449, 0.9555102 , 0.99183673,\n",
       "       0.96      , 0.95959184, 0.99836735, 0.99469388, 0.99705882])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier() # 随机森林分类器\n",
    "result = cross_val_score(estimator=model,X=X_train,y=Y_train,cv=10,scoring='roc_auc')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "roc_auc的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9734258370014672"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，不调参，10折交叉验证的AUC平均值是0.9734258370014672"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用贝叶斯优化来调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造优化的目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先定义一个目标函数，里面放入我们希望优化的值。比如，目标函数的输入为随机森林的所有参数，输出为模型10折交叉验证的AUC均值，AUC均值便是我们希望优化的值。\n",
    "\n",
    "由于bayesian-optimization库 **只支持最大值**，所以**最后的输出如果越小越好，那么需要在前面加上负号，以转为最大值**。\n",
    "\n",
    "bayesian-optimization 只能**优化连续的超参数**，因此如果模型需要输入**离散的超参数**，我们便需要将其转为**整数int()**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target(n_estimators,min_samples_split,max_features,max_depth):\n",
    "    target_value = cross_val_score(\n",
    "      RandomForestClassifier(n_estimators=int(n_estimators),\n",
    "                             min_samples_split=int(min_samples_split),\n",
    "                             max_features=min(max_features,0.999),# float\n",
    "                             max_depth=int(max_depth),\n",
    "                             random_state=2\n",
    "                            ),\n",
    "      X_train, # 训练数据\n",
    "      Y_train, # 类别\n",
    "      scoring='roc_auc', # 以AUC来评估每轮交叉验证的结果\n",
    "      cv=10 # 10折交叉验证\n",
    "    ).mean() # 将 10折交叉验证每轮AUC的均值 作为待优化的值\n",
    "    return target_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调参的参数范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'n_estimators': (10, 250),\n",
    "          'min_samples_split': (2, 25),\n",
    "          'max_features': (0.1, 0.999),\n",
    "          'max_depth': (5, 15)\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例化一个bayesian-optimization对象:bayes_opt.BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bayes_opt.BayesianOptimization(f,pbounds,random_state)**参数：\n",
    "1. 第一个参数f,为我们定义的目标函数\n",
    "2. 第二个参数pbounds,为我们需要调参的参数及其范围\n",
    "3. 参数random_state为了重现结果，比如，random_state=1\n",
    "3. 超参数的名称必须和目标函数的输入参数名称一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "model_bo = BayesianOptimization(\n",
    "  f=target,\n",
    "  pbounds=params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出贝叶斯优化过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BayesianOptimization().maximize(init_points,n_iter)\n",
    "1. init_points:指定你想要执行多少个随机探索步骤。,随机探索可以通过多样化勘探空间为贝叶斯优化提供助力\n",
    "2. n_iter:指定你想要执行多少个贝叶斯优化步骤，执行步骤越多，就越可能找到较优的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | max_fe... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9794  \u001b[0m | \u001b[0m 8.045   \u001b[0m | \u001b[0m 0.3973  \u001b[0m | \u001b[0m 8.758   \u001b[0m | \u001b[0m 157.8   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9682  \u001b[0m | \u001b[0m 5.655   \u001b[0m | \u001b[0m 0.4242  \u001b[0m | \u001b[0m 4.156   \u001b[0m | \u001b[0m 194.6   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9813  \u001b[0m | \u001b[95m 9.174   \u001b[0m | \u001b[95m 0.7775  \u001b[0m | \u001b[95m 3.198   \u001b[0m | \u001b[95m 45.0    \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.98    \u001b[0m | \u001b[0m 14.06   \u001b[0m | \u001b[0m 0.6274  \u001b[0m | \u001b[0m 14.97   \u001b[0m | \u001b[0m 237.5   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9647  \u001b[0m | \u001b[0m 5.57    \u001b[0m | \u001b[0m 0.1444  \u001b[0m | \u001b[0m 4.956   \u001b[0m | \u001b[0m 222.4   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9566  \u001b[0m | \u001b[0m 5.459   \u001b[0m | \u001b[0m 0.9035  \u001b[0m | \u001b[0m 24.81   \u001b[0m | \u001b[0m 11.03   \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.9826  \u001b[0m | \u001b[95m 14.99   \u001b[0m | \u001b[95m 0.9655  \u001b[0m | \u001b[95m 2.436   \u001b[0m | \u001b[95m 112.9   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9633  \u001b[0m | \u001b[0m 5.166   \u001b[0m | \u001b[0m 0.8639  \u001b[0m | \u001b[0m 24.98   \u001b[0m | \u001b[0m 100.8   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9753  \u001b[0m | \u001b[0m 14.96   \u001b[0m | \u001b[0m 0.5915  \u001b[0m | \u001b[0m 2.064   \u001b[0m | \u001b[0m 11.18   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.9667  \u001b[0m | \u001b[0m 6.007   \u001b[0m | \u001b[0m 0.7641  \u001b[0m | \u001b[0m 24.89   \u001b[0m | \u001b[0m 249.5   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9757  \u001b[0m | \u001b[0m 14.98   \u001b[0m | \u001b[0m 0.2856  \u001b[0m | \u001b[0m 24.91   \u001b[0m | \u001b[0m 193.0   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.9666  \u001b[0m | \u001b[0m 5.04    \u001b[0m | \u001b[0m 0.5212  \u001b[0m | \u001b[0m 2.394   \u001b[0m | \u001b[0m 10.33   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.9824  \u001b[0m | \u001b[0m 14.99   \u001b[0m | \u001b[0m 0.8864  \u001b[0m | \u001b[0m 2.382   \u001b[0m | \u001b[0m 249.4   \u001b[0m |\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m 0.9836  \u001b[0m | \u001b[95m 14.92   \u001b[0m | \u001b[95m 0.15    \u001b[0m | \u001b[95m 2.09    \u001b[0m | \u001b[95m 157.8   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.9829  \u001b[0m | \u001b[0m 14.82   \u001b[0m | \u001b[0m 0.1023  \u001b[0m | \u001b[0m 3.058   \u001b[0m | \u001b[0m 65.39   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.9832  \u001b[0m | \u001b[0m 14.99   \u001b[0m | \u001b[0m 0.9799  \u001b[0m | \u001b[0m 3.045   \u001b[0m | \u001b[0m 159.8   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.9761  \u001b[0m | \u001b[0m 14.97   \u001b[0m | \u001b[0m 0.2557  \u001b[0m | \u001b[0m 22.15   \u001b[0m | \u001b[0m 249.3   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.9806  \u001b[0m | \u001b[0m 14.85   \u001b[0m | \u001b[0m 0.8227  \u001b[0m | \u001b[0m 2.061   \u001b[0m | \u001b[0m 43.05   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.9671  \u001b[0m | \u001b[0m 5.079   \u001b[0m | \u001b[0m 0.7658  \u001b[0m | \u001b[0m 2.018   \u001b[0m | \u001b[0m 88.06   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.9758  \u001b[0m | \u001b[0m 14.99   \u001b[0m | \u001b[0m 0.1997  \u001b[0m | \u001b[0m 22.08   \u001b[0m | \u001b[0m 138.1   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.9751  \u001b[0m | \u001b[0m 14.65   \u001b[0m | \u001b[0m 0.1125  \u001b[0m | \u001b[0m 21.09   \u001b[0m | \u001b[0m 51.91   \u001b[0m |\n",
      "| \u001b[95m 22      \u001b[0m | \u001b[95m 0.984   \u001b[0m | \u001b[95m 14.98   \u001b[0m | \u001b[95m 0.3107  \u001b[0m | \u001b[95m 2.171   \u001b[0m | \u001b[95m 134.0   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.9816  \u001b[0m | \u001b[0m 14.92   \u001b[0m | \u001b[0m 0.9927  \u001b[0m | \u001b[0m 8.115   \u001b[0m | \u001b[0m 149.1   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.9799  \u001b[0m | \u001b[0m 14.94   \u001b[0m | \u001b[0m 0.3305  \u001b[0m | \u001b[0m 10.06   \u001b[0m | \u001b[0m 174.0   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.9834  \u001b[0m | \u001b[0m 14.1    \u001b[0m | \u001b[0m 0.1374  \u001b[0m | \u001b[0m 2.269   \u001b[0m | \u001b[0m 37.5    \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.9839  \u001b[0m | \u001b[0m 14.97   \u001b[0m | \u001b[0m 0.1085  \u001b[0m | \u001b[0m 2.333   \u001b[0m | \u001b[0m 52.88   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.9832  \u001b[0m | \u001b[0m 14.97   \u001b[0m | \u001b[0m 0.1611  \u001b[0m | \u001b[0m 2.009   \u001b[0m | \u001b[0m 141.6   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.9837  \u001b[0m | \u001b[0m 14.99   \u001b[0m | \u001b[0m 0.1404  \u001b[0m | \u001b[0m 2.304   \u001b[0m | \u001b[0m 49.77   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.9835  \u001b[0m | \u001b[0m 14.97   \u001b[0m | \u001b[0m 0.1757  \u001b[0m | \u001b[0m 2.311   \u001b[0m | \u001b[0m 130.0   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.9828  \u001b[0m | \u001b[0m 14.96   \u001b[0m | \u001b[0m 0.9417  \u001b[0m | \u001b[0m 2.254   \u001b[0m | \u001b[0m 135.3   \u001b[0m |\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "model_bo.maximize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看当前最优的参数和结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BayesianOptimization in module bayes_opt.bayesian_optimization object:\n",
      "\n",
      "class BayesianOptimization(Observable)\n",
      " |  Inspired/Taken from\n",
      " |      https://www.protechtraining.com/blog/post/879#simple-observer\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BayesianOptimization\n",
      " |      Observable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, f, pbounds, random_state=None, verbose=2)\n",
      " |  \n",
      " |  maximize(self, init_points=5, n_iter=25, acq='ucb', kappa=2.576, xi=0.0, **gp_params)\n",
      " |      Mazimize your function\n",
      " |  \n",
      " |  probe(self, params, lazy=True)\n",
      " |      Probe target of x\n",
      " |  \n",
      " |  register(self, params, target)\n",
      " |      Expect observation with known target\n",
      " |  \n",
      " |  set_bounds(self, new_bounds)\n",
      " |      A method that allows changing the lower and upper searching bounds\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      new_bounds : dict\n",
      " |          A dictionary with the parameter name and its new bounds\n",
      " |  \n",
      " |  set_gp_params(self, **params)\n",
      " |  \n",
      " |  suggest(self, utility_function)\n",
      " |      Most promissing point to probe next\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  max\n",
      " |  \n",
      " |  res\n",
      " |  \n",
      " |  space\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Observable:\n",
      " |  \n",
      " |  dispatch(self, event)\n",
      " |  \n",
      " |  get_subscribers(self, event)\n",
      " |  \n",
      " |  subscribe(self, event, subscriber, callback=None)\n",
      " |  \n",
      " |  unsubscribe(self, event, subscriber)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Observable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model_bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'max_depth': 14.976600931479426,\n",
       "  'max_features': 0.31070824026844507,\n",
       "  'min_samples_split': 2.170749582518546,\n",
       "  'n_estimators': 134.02010184414982},\n",
       " 'target': 0.9839791916766707}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bo.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 寻找已知较优参数附近更优的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果已知有一组或是几组参数是不错的参数组合，但我们想知道该参数组合附近是否还存在更好的参数，便可以利用贝叶斯优化中的explore(探索)操作，而bayesian-optimization库则实现了此方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "known_params = {'n_estimators': [200, 250, 300],\n",
    "        'min_samples_split': [2, 10, 20],\n",
    "        'max_features': [0.93, 0.94, 0.98],\n",
    "        'max_depth': [14, 15, 16]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将上面三组较优的超参数添加到BayesianOptimization()对象中，让其在该参数基础上进行probe(探索的含义),有可能得到更好的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_bo.probe(known_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | max_fe... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.9826  \u001b[0m | \u001b[0m 14.17   \u001b[0m | \u001b[0m 0.7007  \u001b[0m | \u001b[0m 7.003   \u001b[0m | \u001b[0m 170.7   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.9828  \u001b[0m | \u001b[0m 12.06   \u001b[0m | \u001b[0m 0.6026  \u001b[0m | \u001b[0m 7.351   \u001b[0m | \u001b[0m 237.5   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.9696  \u001b[0m | \u001b[0m 6.211   \u001b[0m | \u001b[0m 0.7203  \u001b[0m | \u001b[0m 21.64   \u001b[0m | \u001b[0m 12.17   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.975   \u001b[0m | \u001b[0m 12.16   \u001b[0m | \u001b[0m 0.7125  \u001b[0m | \u001b[0m 23.36   \u001b[0m | \u001b[0m 45.45   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.9775  \u001b[0m | \u001b[0m 9.127   \u001b[0m | \u001b[0m 0.5316  \u001b[0m | \u001b[0m 16.68   \u001b[0m | \u001b[0m 130.0   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 0.9824  \u001b[0m | \u001b[0m 12.35   \u001b[0m | \u001b[0m 0.4093  \u001b[0m | \u001b[0m 8.352   \u001b[0m | \u001b[0m 32.12   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.9825  \u001b[0m | \u001b[0m 14.79   \u001b[0m | \u001b[0m 0.6114  \u001b[0m | \u001b[0m 4.886   \u001b[0m | \u001b[0m 63.17   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.9744  \u001b[0m | \u001b[0m 12.37   \u001b[0m | \u001b[0m 0.5047  \u001b[0m | \u001b[0m 24.53   \u001b[0m | \u001b[0m 116.2   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.9804  \u001b[0m | \u001b[0m 9.575   \u001b[0m | \u001b[0m 0.4222  \u001b[0m | \u001b[0m 7.102   \u001b[0m | \u001b[0m 220.5   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.9709  \u001b[0m | \u001b[0m 6.832   \u001b[0m | \u001b[0m 0.5805  \u001b[0m | \u001b[0m 15.33   \u001b[0m | \u001b[0m 150.4   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.982   \u001b[0m | \u001b[0m 14.99   \u001b[0m | \u001b[0m 0.6889  \u001b[0m | \u001b[0m 2.11    \u001b[0m | \u001b[0m 210.3   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.9807  \u001b[0m | \u001b[0m 14.95   \u001b[0m | \u001b[0m 0.948   \u001b[0m | \u001b[0m 13.68   \u001b[0m | \u001b[0m 212.8   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.9822  \u001b[0m | \u001b[0m 14.91   \u001b[0m | \u001b[0m 0.8284  \u001b[0m | \u001b[0m 2.296   \u001b[0m | \u001b[0m 231.8   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.9723  \u001b[0m | \u001b[0m 14.96   \u001b[0m | \u001b[0m 0.1022  \u001b[0m | \u001b[0m 17.59   \u001b[0m | \u001b[0m 17.61   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.9613  \u001b[0m | \u001b[0m 5.194   \u001b[0m | \u001b[0m 0.116   \u001b[0m | \u001b[0m 17.42   \u001b[0m | \u001b[0m 46.84   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.9731  \u001b[0m | \u001b[0m 14.97   \u001b[0m | \u001b[0m 0.5807  \u001b[0m | \u001b[0m 24.68   \u001b[0m | \u001b[0m 82.43   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.9634  \u001b[0m | \u001b[0m 5.496   \u001b[0m | \u001b[0m 0.1037  \u001b[0m | \u001b[0m 2.173   \u001b[0m | \u001b[0m 123.0   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.9626  \u001b[0m | \u001b[0m 5.238   \u001b[0m | \u001b[0m 0.8299  \u001b[0m | \u001b[0m 25.0    \u001b[0m | \u001b[0m 182.5   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.98    \u001b[0m | \u001b[0m 14.86   \u001b[0m | \u001b[0m 0.1359  \u001b[0m | \u001b[0m 9.807   \u001b[0m | \u001b[0m 96.29   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.9801  \u001b[0m | \u001b[0m 13.95   \u001b[0m | \u001b[0m 0.2142  \u001b[0m | \u001b[0m 9.336   \u001b[0m | \u001b[0m 249.8   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.9752  \u001b[0m | \u001b[0m 12.04   \u001b[0m | \u001b[0m 0.1877  \u001b[0m | \u001b[0m 24.89   \u001b[0m | \u001b[0m 219.3   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.982   \u001b[0m | \u001b[0m 14.79   \u001b[0m | \u001b[0m 0.9039  \u001b[0m | \u001b[0m 2.322   \u001b[0m | \u001b[0m 83.14   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.9783  \u001b[0m | \u001b[0m 10.92   \u001b[0m | \u001b[0m 0.9152  \u001b[0m | \u001b[0m 2.079   \u001b[0m | \u001b[0m 24.33   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.9835  \u001b[0m | \u001b[0m 14.95   \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 2.008   \u001b[0m | \u001b[0m 187.9   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.9798  \u001b[0m | \u001b[0m 14.91   \u001b[0m | \u001b[0m 0.1292  \u001b[0m | \u001b[0m 9.78    \u001b[0m | \u001b[0m 124.7   \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 0.9823  \u001b[0m | \u001b[0m 10.43   \u001b[0m | \u001b[0m 0.3461  \u001b[0m | \u001b[0m 2.047   \u001b[0m | \u001b[0m 172.3   \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.9813  \u001b[0m | \u001b[0m 14.75   \u001b[0m | \u001b[0m 0.128   \u001b[0m | \u001b[0m 8.142   \u001b[0m | \u001b[0m 224.4   \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.9836  \u001b[0m | \u001b[0m 14.99   \u001b[0m | \u001b[0m 0.3076  \u001b[0m | \u001b[0m 2.226   \u001b[0m | \u001b[0m 98.05   \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.982   \u001b[0m | \u001b[0m 13.92   \u001b[0m | \u001b[0m 0.9889  \u001b[0m | \u001b[0m 10.45   \u001b[0m | \u001b[0m 193.5   \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 0.979   \u001b[0m | \u001b[0m 7.824   \u001b[0m | \u001b[0m 0.9051  \u001b[0m | \u001b[0m 2.027   \u001b[0m | \u001b[0m 249.6   \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m 0.9816  \u001b[0m | \u001b[0m 12.24   \u001b[0m | \u001b[0m 0.8708  \u001b[0m | \u001b[0m 2.184   \u001b[0m | \u001b[0m 152.0   \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m 0.9828  \u001b[0m | \u001b[0m 14.06   \u001b[0m | \u001b[0m 0.9786  \u001b[0m | \u001b[0m 2.072   \u001b[0m | \u001b[0m 182.6   \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m 0.9837  \u001b[0m | \u001b[0m 12.87   \u001b[0m | \u001b[0m 0.1077  \u001b[0m | \u001b[0m 2.056   \u001b[0m | \u001b[0m 240.5   \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m 0.9829  \u001b[0m | \u001b[0m 13.11   \u001b[0m | \u001b[0m 0.9926  \u001b[0m | \u001b[0m 2.042   \u001b[0m | \u001b[0m 60.06   \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m 0.9809  \u001b[0m | \u001b[0m 14.96   \u001b[0m | \u001b[0m 0.1013  \u001b[0m | \u001b[0m 8.824   \u001b[0m | \u001b[0m 200.5   \u001b[0m |\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "model_bo.maximize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新参数为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'max_depth': 14.976600931479426,\n",
       "  'max_features': 0.31070824026844507,\n",
       "  'min_samples_split': 2.170749582518546,\n",
       "  'n_estimators': 134.02010184414982},\n",
       " 'target': 0.9839791916766707}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bo.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 观察 贝叶斯优化获得的参数 对模型性能的提升效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9839791916766707"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=int(134.02010184414982),\n",
    "                             min_samples_split=int(2.170749582518546),\n",
    "                             max_features=min(0.31070824026844507,0.999),# float\n",
    "                             max_depth=int(14.976600931479426),\n",
    "                             random_state=2\n",
    "                            ) # 随机森林分类器\n",
    "result = cross_val_score(estimator=model,X=X_train,y=Y_train,cv=10,scoring='roc_auc')\n",
    "result.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比未使用**贝叶斯优化**的结果0.9734258370014672，提高了0.01，提升效果很明显"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
