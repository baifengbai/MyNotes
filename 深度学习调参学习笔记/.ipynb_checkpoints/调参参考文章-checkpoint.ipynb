{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 两类需要调参的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文学习于[10分钟教你深度学习的调参](https://www.jianshu.com/p/a0009ec5225e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化类的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率：learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个好的起点是从**0.01**开始尝试，可选的几个常用值:\n",
    "1. 0.01\n",
    "2. 0.001\n",
    "3. 0.0001\n",
    "4. 0.00001\n",
    "5. 0.000001\n",
    "\n",
    "判断依据是验证集的误差(validation error)\n",
    "\n",
    "常用策略：学习率衰减\n",
    "\n",
    "如果选用了Adam和Adagrad的作为优化器（optimizer），则他们自带了可自适应的学习率（adaptive learning rate）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练批量的大小：mini batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可选的常用值:1,2,4,8,16,32,64,128,256\n",
    "\n",
    "推荐**32**比较常用。32,64,128,256都比较合适。\n",
    "\n",
    "256比较大，一次性计算的多，速度会快，但因为矩阵计算量较大，内存可能过载\n",
    "\n",
    "小的mini batch size可能因为收敛的抖动比较厉害反而不容易卡在局部最低点；但mini batch也不能太大，反而准确率下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 迭代次数：Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择合适的Epochs，就可以用early stopping的方法：\n",
    "如果验证集的误差上升，就early stopping；但别一看到上升就停，再观察一下，因为有可能只是暂时的现象，这时停止反而训练会不充分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型类的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐藏层数:hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 隐藏层神经元数hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决的问题越复杂，则用越多的hidden units，但过多，反而会过拟合；增加的hidden units数量直到验证集误差上升为止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 隐藏层的层数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常来说，3层的隐藏层比2层的好，但4,5,6层再深就没什么效果了，例外情况是CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN的参数：\n",
    "1. 选择cell类型，常用LSTM和GRU\n",
    "2. stack通常两层，即两个RNN堆叠成的多层RNN \n",
    "\n",
    "用作RNN模型输入的word embeddings层的embedding数量控制在50-200之间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习网络调参技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文学习于[深度学习网络调参技巧](https://zhuanlan.zhihu.com/p/24720954)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 好的实验环境是成功的一半"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于深度学习实验超参众多，代码风格良好的实验环境，可以让你的人工或者自动调参更加省力，有以下几点可能需要注意：\n",
    "1. 将各个参数的设置集中放在一起。如果参数的设置分布在代码的各个地方，那么修改的过程想必会非常痛苦\n",
    "2. 可以输出模型的损失函数值以及训练集和验证集上的准确率\n",
    "3. 可以考虑设计一个子程序，根据给定的参数，启动训练并监控和周期性保存评估结果，再由一个主程序，分配参数以及并行启动一系列子程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画图是一个很好的习惯，一般是训练数据遍历一轮后，就输出一下训练集和验证集准确率，同时画到一张图上。这样训练一段时间以后，如果模型一直没有收敛，那么就可以停止训练，去尝试其他参数，以节省时间。\n",
    "\n",
    "如果训练到最后，训练集和测试集准确率都很低，那么说明模型有可能欠拟合。那么后续调节参数方向，就是增强模型的拟合能力。例如，增加网络层数，增加节点，减少dropout值，减少L2正则值等等。\n",
    "\n",
    "如果训练集准确率高，测试集准确率底，那么模型有可能过拟合，这个时候就需要提高模型泛化能力的方向，调节参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从粗到细分阶段调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实践中，一般先进行初步范围搜索，然后根据好结果出现的地方，再缩小范围进行更精细的搜索：\n",
    "1. 建议先参考相关论文，以论文中给出的参数作为初始参数。至少论文中的参数，是个不差的结果\n",
    "2. 如果找不到参考，那么只能自己尝试了。可以先从比较重要，对实验结果影响比较大的参数开始，同时固定其他参数，得到一个差不多的结果以后，在这个结果的基础上，再调其他参数。例如，如学习率一般就比正则值、dropout值重要的话，学习率设置的不合适，不仅结果可能变差，模型甚至会无法收敛\n",
    "3. 如果实在找不到一组参数可以让模型收敛，那么就需要检查，是不是其他地方出了问题，例如，模型实现，数据等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提高速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调参只是为了寻找合适的参数，而不是产出最终模型。一般在小数据集上合适的参数，在大数据集上效果也不会太差。因此可以尝试对数据进行精简，在有限的时间内可以尝试更多参数。\n",
    "\n",
    "1. 对训练数据进行采样，例如，原来100W条数据，先采样成1W，进行实验看看\n",
    "2. 减少训练类别，例如，手写数字识别任务，原来是10个类别，那么我们可以先在2个类别上训练，看看结果如何"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数范围"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建议优先在对数尺度上进行超参数搜索。比较典型的是学习率和正则化项，我们可以从诸如0.001 0.01 0.1 1 10，以10为阶数进行尝试。因为它们对训练的影响是相乘的效果。不过有些参数，还是建议在原始尺度上进行搜索，例如dropout值：0.3  0.5  0.7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 经验参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里给出一些参数的经验值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 1,0.1,0.01,0.001，一般从1开始尝试。很少见learning rate大于10的\n",
    "2. 学习率一般要随着训练进行衰减。衰减系数一般是0.5。\n",
    "3. 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期后\n",
    "4. 更建议使用自适应梯度的办法，例如Adam,Adadelta,Rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。\n",
    "5. 对RNN来说，有个经验，如果要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络层数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先从1层开始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 每层节点数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 32 128 ，超过1000的情况比较少见。超过1W的从来没见过"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clip(梯度裁剪)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度裁剪是一种在非常深的网络（通常是循环神经网络）中用于**防止梯度爆炸（exploding gradient）**的技术。 \n",
    "![梯度裁剪](./image/gradient_clip.jpg)\n",
    "\n",
    "执行梯度裁剪的方法有很多，但常见的一种是当参数的 L2 范数（L2 norm）超过一个特定阈值时对**参数的梯 \n",
    "度进行标准化**，根据函数计算新梯度:新梯度 = 梯度 * 阈值 / 梯度L2范数 即 new_gradients = gradients * threshold / l2_norm(gradients)，**新梯度取值为$min(new_gradients,threshold)$**。\n",
    "\n",
    "梯度裁剪就是限制最大梯度，其实是$value = \\sqrt{w_{1}^{2} + w_{2}^{2} +...+ w_{n}^{2}}$，如果梯度的value超过指定的阈值，就算一个衰减系数，以让梯度的值等于阈值:5,10,15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2正则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.0，超过10的很少见"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量embedding大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "128,256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正负样本比例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个非常忽视，但是在很多分类问题上，又是非常重要的参数。很多人往往习惯使用训练数据中默认的正负类别比例，当训练数据非常不平衡的时候，模型很有可能会偏向数目较大的类别，从而影响最终训练结果。除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样，例如进行复制。提高他们的比例，看看效果如何，这个对多分类问题同样适用。 \n",
    "\n",
    "在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人工一直盯着实验，毕竟太累。自动调参当前也有不少研究，下面介绍几种比较实用的办法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search:网格搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个是最常见的。具体说，就是每种参数确定好几个要尝试的值，然后像一个网格一样，把所有参数值的组合遍历一下。优点是实现简单暴力，如果能全部遍历的话，结果比较可靠。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search：随机搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bengio在Random Search for Hyper-Parameter Optimization中指出，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization：贝叶斯优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯优化，考虑到了不同参数对应的实验结果值，因此更节省时间。和网络搜索相比简直就是老牛和跑车的区别。具体原理可以参考这个论文： Practical Bayesian Optimization of Machine Learning Algorithms ，这里同时推荐两个实现了贝叶斯调参的Python库，可以上手即用：\n",
    "1. jaberg/hyperopt, 比较简单\n",
    "2. fmfn/BayesianOptimization， 比较复杂，支持并行调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 合理性检查，确定模型，数据和其他地方没有问题\n",
    "2. 训练时跟踪损失函数值，训练集和验证集准确率\n",
    "3. 使用Random Search来搜索最优超参数，分阶段从粗（较大超参数范围训练较少周期）到细（较小超参数范围训练较长周期）进行搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习网络调试技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文学习于[深度学习网络调试技巧](https://zhuanlan.zhihu.com/p/20792837)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 遇到Nan怎么办？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nan问题，我相信大部分人都遇到过，一般可能是下面几个原因造成的：\n",
    "\n",
    "1. 除0问题。这里实际上有两种可能，一种是被除数的值是无穷大，即Nan，另一种就是除数的值是0。之前产生的Nan或者0，有可能会被传递下去，造成后面都是Nan。请先检查一下神经网络中有可能会有除法的地方，例如softmax层，再认真的检查一下数据。我有一次帮别人调试代码，甚至还遇到过，训练数据文件中，有些值就是Nan。。。这样读进来以后，开始训练，只要遇到Nan的数据，后面也就Nan了。可以尝试加一些日志，把神经网络的中间结果输出出来，看看哪一步开始出现Nan。\n",
    "2. 梯度过大，造成更新后的值为Nan。特别是RNN，在序列比较长的时候，很容易出现梯度爆炸的问题。一般有以下几个解决办法。\n",
    "    1. 对梯度做clip(梯度裁剪），限制最大梯度,其实是$value = \\sqrt{w_{1}^{2} + w_{2}^{2} +...+ w_{n}^{2}}$,如果value超过了阈值,就算一个衰减系系数,让value的值小于等于阈值: 5,10,15。\n",
    "    2. 减少学习率。初始学习率过大，也有可能造成这个问题。需要注意的是，即使使用adam之类的自适应学习率算法进行训练，也有可能遇到学习率过大问题，而这类算法，一般也有一个学习率的超参，可以把这个参数改的小一些。\n",
    "    3. 初始参数值过大，也有可能出现Nan问题。输入和输出的值，最好也做一下归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络学不出东西怎么办？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可能我们并没有遇到，或者解决了Nan等问题，网络一直在正常的训练，但是cost降不下来，预测的时候，结果不正常。\n",
    "\n",
    "1. 请打印出训练集的cost值和测试集上cost值的变化趋势，正常情况应该是训练集的cost值不断下降，最后趋于平缓，或者小范围震荡，测试集的cost值先下降，然后开始震荡或者慢慢上升。如果训练集cost值不下降，有可能是代码有bug，有可能是数据有问题（本身有问题，数据处理有问题等等），有可能是超参（网络大小，层数，学习率等）设置的不合理。 请人工构造10条数据，用神经网络反复训练，看看cost是否下降，如果还不下降，那么可能网络的代码有bug，需要认真检查了。如果cost值下降，在这10条数据上做预测，看看结果是不是符合预期。那么很大可能网络本身是正常的。那么可以试着检查一下超参和数据是不是有问题。\n",
    "2. 如果神经网络代码，全部是自己实现的，那么强烈建议做梯度检查。确保梯度计算没有错误。\n",
    "3. 先从最简单的网络开始实验，不要仅仅看cost值，还要看一看神经网络的预测输出是什么样子，确保能跑出预期结果。例如做语言模型实验的时候，先用一层RNN，如果一层RNN正常，再尝试LSTM，再进一步尝试多层LSTM。\n",
    "4. 如果可能的话，可以输入一条指定数据，然后自己计算出每一步正确的输出结果，再检查一下神经网络每一步的结果，是不是一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习如何调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文学习于[深度学习如何调参](https://zhuanlan.zhihu.com/p/34209099)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调参的关键是**快速尝试，快速纠错**，快速试错。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关于可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练目标可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于可视化,更多的还是帮助人类以自己熟悉的方式来观察网络. 因为,你是不可能边观察网络, 还边调参的. 你只是训练完成后(或者准确率到达一个阶段后), 才能可视化. 在这之前, 网络没有学习到良好的参数, 你可视化了也没意义, 网络达到不错的准确率了, 你看看其实也就听个响. 同样, 你的网络训练的一塌糊涂, 你可视化也没什么意义, 唯一能够看到的就是中间结果乱七八糟, 这时候你直接看最后准确率就可以知道这网络没救了."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 权重可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Layer Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss设计要合理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 一般来说分类就是Softmax, 回归就是L2的loss. 但是要注意loss的错误范围(主要是回归), 你预测一个label是10000的值, 模型输出0, 你算算这loss多大, 这还是单变量的情况下. 一般结果都是nan. 所以不仅仅输入要做normalization, 输出也要这么弄.\n",
    "2. 多任务情况下, 各loss想办法限制在一个量级上, 或者最终限制在一个量级上, 初期可以着重一个任务的loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 观察loss胜于观察准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 准确率虽然是评测指标, 但是训练过程中还是要注意loss的. 你会发现有些情况下, 准确率是突变的, 原来一直是0, 可能保持上千迭代, 然后突然变1. 要是因为这个你提前中断训练了, 只有老天替你惋惜了. 而loss是不会有这么诡异的情况发生的, 毕竟优化目标是loss\n",
    "2. 给NN一点时间, 要根据任务留给NN的学习一定空间. 不能说前面一段时间没起色就不管了. 有些情况下就是前面一段时间看不出起色, 然后开始稳定学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 确认分类网络学习充分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类网络就是学习类别之间的界限. 你会发现, 网络就是慢慢的从类别模糊到类别清晰的. 怎么发现? 看Softmax输出的概率的分布. 如果是二分类, 你会发现, 刚开始的网络预测都是在0.5上下, 很模糊. 随着学习过程, 网络预测会慢慢的移动到0,1这种极值附近. 所以, 如果你的网络预测分布靠中间, 再学习学习."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning rate设置合理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 太大: loss爆炸, 或者nan\n",
    "2. 太小: 半天loss没反映\n",
    "3. 需要进一步降低了: loss在当前LR下一路降了下来, 但是半天不再降了.\n",
    "4. 如果有个复杂点的任务, 刚开始, 是需要人肉盯着调LR的. 后面熟悉这个任务网络学习的特性后, 可以扔一边跑去了.\n",
    "5. 如果上面的Loss设计那块你没法合理, 初始情况下容易爆, 先上一个小LR保证不爆, 等loss降下来了, 再慢慢升LR, 之后当然还会慢慢再降LR, 虽然这很蛋疼.\n",
    "6. LR在可以工作的最大值下往小收一收, 免得ReLU把神经元弄死了. 当然, 我是个心急的人, 总爱设个大点的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比训练集和验证集的loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "判断过拟合, 训练是否足够, 是否需要early stop的依据, 这都是中规中矩的原则"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
