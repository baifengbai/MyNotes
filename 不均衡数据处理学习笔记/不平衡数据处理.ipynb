{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文学习于[普林科技.不平衡数据处理](https://mp.weixin.qq.com/s?__biz=MzAwMzIxMjIyMg==&mid=2651005812&idx=1&sn=b9819f04cb2ee9af21f4011d34013824&scene=0)和[不平衡数据下的机器学习方法简介](https://www.jianshu.com/p/3e8b9f2764c8),编辑:Weiyang,Time:2019.2.8,weixin:damo894127201"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用的分类算法一般假设不同类的比例是均衡的，现实生活中经常遇到**不平衡的数据集**，比如广告点击预测(点击转化率一般都很小)、商品推荐（推荐的商品被购买的比例很低）、信用卡欺诈检测等。\n",
    "\n",
    "对于不平衡数据集，一般的分类算法都倾向于将样本划分到多数类，体现在模型整体的准确率很高。\n",
    "\n",
    "但对于极不均衡的分类问题，比如仅有1%的人是坏人，99%的人是好人，最简单的分类模型就是将所有人都划分为好人，模型都能得到99%的准确率，显然这样的模型并没有提供任何的信息。\n",
    "\n",
    "在类别不平衡的情况下，对模型使用**F值**或者**AUC值**是更好的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理不平衡数据的方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改变数据分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从数据层面使得类别更为均衡，方法主要是**重采样**:\n",
    "1. 欠采样:减少多数类样本的数量\n",
    "2. 过采样:增加少数类样本的数量\n",
    "3. 综合采样:将欠采样和过采样结合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改变分类算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在传统分类算法的基础上，对不同类别的损失采取不同的权重，使得模型更看重少数类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under-sampling:欠采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机欠采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "减少多数类样本数量最简单的方法便是**随机剔除多数类样本**，可以事先设置多数类与少数类最终的数量比例ratio，在保留少数类样本不变的情况下，根据ratio随机选择多数类样本。\n",
    "\n",
    "1. 优点:操作简单，只依赖于样本分布，不依赖于任何距离信息，属于非启发式方法\n",
    "2. 缺点:会丢失一部分多数类样本的信息，无法充分利用已有信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tomek link方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果两个不同类别的样本，它们的最近邻都是对方，也就是样本A的最近邻是样本B，样本B的最近邻是样本A，那么样本A，B就是**Tomek link对**。容易看出，如果两个样本为Tomek link对，则其中某个样本为噪声(偏离正常分布太多)或者两个样本都在两类的边界上。\n",
    "\n",
    "Tomek link对的用途:\n",
    "1. 欠采样:将Tomek link多数类的样本剔除\n",
    "2. 数据清洗:将Tomek link对中的两个样本都剔除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Near Miss方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NearMiss方法是利用**距离远近剔除多数类样本**的一类方法，实际操作中也是借助kNN，总结起来有以下几类:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearMiss-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在多数类样本中，选择与**最近**的3个少数类样本的平均距离最小的样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearMiss-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在多数类样本中，选择与**最远**的3个少数类样本的平均距离最小的样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearMiss-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每个少数类样本，选择离它最近的给定数量的多数类样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三种方法比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. NearMiss-1和NearMiss-2方法的描述仅有一字之差，但其含义是完全不同的：\n",
    "    1. NearMiss-1考虑的是与最近的3个少数类样本的平均距离，是局部的\n",
    "    2. NearMiss-2考虑的是与最远的3个少数类样本的平均距离，是全局的\n",
    "\n",
    "2. NearMiss-1方法得到的多数类样本分布也是“不均衡”的，它倾向于在比较集中的少数类附近找到更多的多数类样本，而在孤立的（或者说是离群的）少数类附近找到更少的多数类样本，原因是NearMiss-1方法考虑的局部性质和平均距离\n",
    "\n",
    "3. NearMiss-3方法则会使得每一个少数类样本附近都有足够多的多数类样本，显然这会使得模型的精确度高、召回率低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over-sampling:过采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机过采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与欠采样对应，增加少数类样本数量最简单的方法便是**随机复制少数类样本**，可以事先设置多数类与少数类最终的数量比例ratio，在保留多数类样本不变的情况下，根据ratio随机复制少数类样本。\n",
    "\n",
    "在使用过程中为了保证所有的少数类样本信息都会被包含，可以先完全复制一份全量的少数类样本，再随机复制少数类样本使得数量比例满足给定的ratio。\n",
    "\n",
    "1. 优点：操作简单，只依赖于样本分布，不依赖于任何距离信息，属于非启发式方法\n",
    "2. 缺点：重复样本过多，容易造成分类器的过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE全称为Synthetic Minority Over-sampling Technique,即合成少数类过采样技术。由于过采样采取简单复制样本的策略来增加少数类样本，使得模型学习到的信息过于特别specific而不够泛化general,这样容易使模型过拟合。\n",
    "\n",
    "SMOTE的**主要思想**是通过在一些位置相近的少数类样本中人工生成新样本达到平衡类别的目的，由于不是简单地复制少数类样本，因此可以在一定程度上避免分类器的过度拟合。\n",
    "\n",
    "具体如下图所示:\n",
    "![SMOTE](./image/SMOTE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 设置向上采样的倍率为N，即对每个少数类样本都需要产生对应的N个少数类新样本\n",
    "2. 对少数类中的每一个样本$x$，搜索得到其k（通常取5）个少数类最近邻样本，并从中随机选择N个样本，记为$\\widetilde{x}_{1},\\widetilde{x}_{2},…,\\widetilde{x}_{N}$（可能有重复值）\n",
    "3. 构造新的少数类样本 $x_{new} = x + rand(0,1) \\times (\\widetilde{x} − x)$，其中rand(0,1)表示区间(0,1)内的随机数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Borderline SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE算法摒弃了随机过采样复制样本的做法，可以防止随机过采样易过拟合的问题，实践证明此方法可以提高分类器的性能。但是由于对每个少数类样本都生成新样本，故容易发生**生成样本重叠(Overlapping)**的问题.为了解决SMOTE算法的这一缺点，便提出了一些改进的算法，Borderline SMOT便是其一。\n",
    "\n",
    "实际建模过程中发现那些处于边界位置的样本更容易被错分，因此利用**边界位置的样本信息**产生**新样本**可以给模型带来更大的提升。\n",
    "\n",
    "Borderline SMOTE 便是将**原始的SMOTE算法**和**边界信息**结合的算法，它有两个版本:Borderline SMOTE-1 和 Borderline SMOTE-2 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borderline SMOTE-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法流程:\n",
    "1. 记整个训练集合为$T$，其中：少数类样本集合为$P$，多数类样本集合为$N$。对**少数类**$P$中的**每个样本$x_{i}$**，在整个训练集合$T$中搜索得到其最近的$m$个样本，记其中少数类样本数量为$m_{i}$；\n",
    "2. 区分:噪声样本NOISE,安全样本SAFE,危险样本集DANGER\n",
    "    1. 若$m_{i}=0$，即样本$x_{i}$附近$m$个最近的样本均是多数类，则认为样本$x_{i}$为噪声NOISE，不进行任何操作;\n",
    "    2. 若$m_{i}>\\frac{m}{2}$,则认为$x_{i}$为安全样本SAFE，不进行任何操作；\n",
    "    3. 若$0<m_{i}\\leq\\frac{m}{2}$，则认为$x_{i}$处于少数类的边界位置，将其加入危险样本集DANGER；\n",
    "3. 对DANGER中的每一个样本点，采用原始的SMOTE算法生成新的少数类样本；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borderline SMOTE-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borderline SMOTE-2和Borderline SMOTE-1是很类似的，区别是在得到DANGER集合之后，对于DANGER中的每一个样本点$x_{i}$:\n",
    "1. Borderline SMOTE-1\n",
    "    1. 从少数类样本集合$P$中得到k个最近邻样本，再随机选择样本点和$x_{i}$作随机的**线性插值**产生新的少数类样本（和原始的SMOTE算法流程相同）\n",
    "\n",
    "\n",
    "2. Borderline SMOTE-2\n",
    "    1. 从少数类样本集合$P$和多数类样本集合$N$中分别得到样本$x_{i}$的k个最近邻样本$P_{k}$和$N_{k}$。设定一个比例$α$，在$P_{k}$中选出$α$比例的样本点和$x_{i}$作随机的**线性插值**产生新的少数类样本，方法同Borderline SMOTE-1；\n",
    "    2. 在$N_{k}$中选出1−α比例的样本点和$x_{i}$作随机的**线性插值**产生新的少数类样本，此处的随机数范围选择的是(0,0.5)，即使得产生的新的样本点更靠近少数类样本\n",
    "\n",
    "![borderline-smote](./image/borderline-smote.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，上图中五角星表示少数类，空心圆表示多数类。且在此更正一下上图：For C: number of minority instance:0,number of majority instance:6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 综合采样：将欠采样和过采样结合起来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前为止，我们使用的重采样方法几乎都是只针对某一类样本：要么对多数类样本欠采样，要么对少数类样本过采样。将过采样和欠采样综合的方法，来解决样本类别不均衡和过拟合的问题，这里介绍两种:SMOTE+Tomek links 和 SMOTE+ENN 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE+Tomek link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法流程很简单:\n",
    "1. 利用SMOTE算法生成新的少数类样本，得到扩充后的数据集$T$\n",
    "2. 剔除$T$中得分Tomek link对\n",
    "\n",
    "普通SMOTE方法生成的少数类样本是通过线性插值得到的，在平衡类别分布的同时也扩张了少数类的样本空间，**产生的问题**是可能**原本属于多数类样本的空间被少数类“入侵”（invade），容易造成模型的过拟合**。Tomek link对寻找的是那种**噪声点**或者**边界点**，可以很好地解决“入侵”的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE+ENN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法流程:\n",
    "1. 利用SMOTE方法生成新的少数类样本，得到扩充后的数据集$T$\n",
    "2. 对$T$中的每一个样本使用$kNN$（一般k取3）方法预测，若预测结果和实际类别标签不符，则剔除该样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENN:最近邻规则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于随机欠抽样方法未考虑样本的分布情况，采样具有很大的随机性，可能会删除重要的多数类样本信息。针对以上的不足，Wilson 等人提出了一种最近邻规则(edited nearest neighbor: ENN)。\n",
    "\n",
    "1. 基本思想：删除那些类别与其最近的三个近邻样本中的两个或两个以上的样本类别不同的样本\n",
    "2. 缺点：因为大多数的多数类样本的样本附近都是多数类，所以该方法所能删除的多数类样本十分有限"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informed Under-sampling采样技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE用来解决**随机过采样**容易发生的**模型过拟合问题**，Informed undersampling采样技术用来解决**随机欠采样**造成的**数据信息丢失问题**。\n",
    "\n",
    "informed undersampling采样技术主要有两种方法分别是**EasyEnsemble算法**和**BalanceCascade算法**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EasyEnsemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此算法类似于随机森林的Bagging方法，它把数据划分为两部分，分别是多数类样本和少数类样本，对于多数类样本，通过n次有放回抽样生成n份子集，少数类样本分别和这n份样本合并训练一个模型，这样可以得到n个模型，最终的模型是这n个模型预测结果的平均值，算法流程如下：\n",
    "![EasyEnsemble](./image/EasyEnsemble.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法核心思想\n",
    "1. 首先通过从**多数类**中**独立随机抽取**出若干子集\n",
    "2. 将每个子集与**少数类数据**联合起来训练生成多个**基分类器**\n",
    "3. 最终将这些基分类器组合形成一个**集成学习系统**\n",
    "\n",
    "EasyEnsemble每次都独立利用**可放回随机抽样**机制来提取多数类样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BalanceCascade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BalanceCascade是一种**级联算法**，也就是将多个基分类器层次串联起来做集成，下一层次分类器分类上一层次分类器未正确分类的样本，直到正负类样本均衡为止。与EasyEnsemble相比，BalanceCascade会在每一级分类器上判断多数类样本在当前分类器下的分类情况，剔除被正确分类的多数类样本，不断用基分类器来分类多数类，直到多数类样本和少数类样本均衡为止。\n",
    "\n",
    "算法流程如下:\n",
    "![balancecascade](./image/balancecascade.png)\n",
    "\n",
    "BalanceCascade(级联平衡)的方法通过使用分类器来确保那些被错分类的样本在下一次进行子集选取的时候也能被采样到。\n",
    "\n",
    "BalanceCascade算法得到的是一个级联分类器，将若干个强分类器由简单到复杂排列，只有和少数类样本特征比较接近的才有可能输入到后面的分类器，比如边界点，因此能更充分地利用多数类样本的信息，一定程度上解决随机欠采样的信息丢失问题。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
